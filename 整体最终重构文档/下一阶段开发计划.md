# OSCEAN 下一阶段开发计划

## 当前状态总结

### ✅ 已完成模块
- **第5层 - 共享基础库** (`common_utils`): 线程池、日志、配置管理等
- **第3层 - 核心服务层** (`core_services`): 数据访问、元数据、空间处理、插值、CRS转换等

### 🚧 部分完成模块
- **第4层 - 输出生成层** (`output_generation`): 瓦片服务框架已搭建
- **第2层 - 任务调度层** (`workflow_engine`, `task_dispatcher`): 基础框架存在
- **第1层 - 网络服务层** (`network_service`): HTTP服务框架存在

## 第一优先级：完成第2层（任务调度与工作流引擎）

### 目标
实现完整的任务调度和工作流编排能力，为海洋数据自动管理提供基础。

### 具体任务

#### 1. 完善 TaskDispatcher 实现
```cpp
// 需要实现的核心接口
class TaskDispatcherImpl : public TaskDispatcher {
public:
    void processApiRequest(NetworkService::RequestDTO request,
                          std::function<void(NetworkService::ResponseData)> callback) override;
    
    // 专门的数据管理工作流入口
    void processDataManagementRequest(NetworkService::RequestDTO request,
                                    std::function<void(NetworkService::ResponseData)> callback);
};
```

#### 2. 实现海洋数据自动管理工作流
基于现有的 `ocean_data_indexer.cpp`，创建工作流定义：

```cpp
// workflow_engine/src/definition/blueprints/data_management_workflow.cpp
class DataManagementWorkflow : public IWorkflowDefinition {
    // 步骤：
    // 1. 扫描数据目录
    // 2. 提取元数据
    // 3. 数据质量检查
    // 4. 更新索引数据库
    // 5. 生成报告
};
```

#### 3. 集成核心服务调用
```cpp
// workflow_engine/src/proxy/core_service_proxy.cpp
class CoreServiceProxy {
public:
    std::future<StepResultData> scanDataDirectory(const std::string& path);
    std::future<StepResultData> extractMetadata(const std::vector<std::string>& files);
    std::future<StepResultData> updateMetadataIndex(const MetadataCollection& metadata);
};
```

### 实施步骤

#### 阶段1：基础框架完善（1-2周）
1. **完善 TaskDispatcher 实现**
   - 实现请求路由逻辑
   - 集成工作流引擎调用
   - 添加错误处理机制

2. **完善 WorkflowExecutor**
   - 修复现有的异步执行逻辑
   - 完善步骤依赖管理
   - 添加工作流状态跟踪

#### 阶段2：数据管理工作流实现（2-3周）
1. **创建数据管理工作流定义**
   ```
   workflow_engine/src/definition/blueprints/
   ├── data_management_workflow.h
   ├── data_management_workflow.cpp
   └── workflow_registry.cpp  # 注册工作流
   ```

2. **实现核心服务代理**
   ```
   workflow_engine/src/proxy/
   ├── core_service_proxy.h
   ├── core_service_proxy.cpp
   └── data_management_proxy.cpp  # 专门的数据管理代理
   ```

3. **集成现有的数据索引功能**
   - 将 `ocean_data_indexer.cpp` 的逻辑拆分为工作流步骤
   - 每个步骤作为独立的可执行单元
   - 支持异步执行和进度报告

#### 阶段3：网络接口集成（1周）
1. **添加数据管理API端点**
   ```cpp
   // 在 RequestRouter 中添加路由
   router->addRoute(http::verb::post, "/api/data/scan", 
       [dispatcher](RequestDTO dto, auto cb) {
           dispatcher->processDataManagementRequest(std::move(dto), std::move(cb));
       });
   ```

2. **实现RESTful API**
   - `POST /api/data/scan` - 启动数据扫描
   - `GET /api/data/scan/{id}` - 查询扫描状态
   - `GET /api/data/reports` - 获取扫描报告

## 第二优先级：完善第4层（输出生成层）

### 目标
完成瓦片服务的实现，支持海洋数据可视化。

### 具体任务

#### 1. 完善瓦片服务实现
- 完成 `TileService::handleTileRequest` 的实现
- 集成缓存机制
- 实现栅格瓦片渲染

#### 2. 集成数据访问
- 连接第3层的数据访问服务
- 实现数据到瓦片的转换流程

## 第三优先级：完善第1层（网络服务层）

### 目标
完成HTTP服务器的完整实现和集成。

### 具体任务

#### 1. 完善网络服务集成
- 完成 `NetworkServer` 与 `TaskDispatcher` 的集成
- 添加配置管理
- 实现优雅关闭机制

#### 2. 添加管理界面支持
- 静态文件服务
- WebSocket支持（用于实时状态更新）

## 海洋环境数据自动管理功能设计

### 功能定位
**属于第2层的工作流应用**，不是独立的app，而是系统内置的数据管理工作流。

### 核心功能
1. **自动数据发现**
   - 监控指定目录的文件变化
   - 识别新增的NetCDF文件
   - 支持递归目录扫描

2. **智能元数据提取**
   - 提取时空范围信息
   - 识别变量和维度
   - 提取数据质量指标

3. **数据质量检查**
   - 文件完整性验证
   - 数据范围合理性检查
   - 时间序列连续性检查

4. **索引管理**
   - 自动更新SQLite索引
   - 支持增量更新
   - 提供快速检索接口

5. **报告生成**
   - 生成数据统计报告
   - 导出CSV详细清单
   - 提供Web界面查看

### 工作流定义示例
```yaml
name: "ocean_data_management"
description: "海洋环境数据自动管理工作流"
steps:
  - id: "scan_directory"
    operation: "data_access.scan_directory"
    inputs:
      path: "${request.data_path}"
    
  - id: "extract_metadata"
    operation: "metadata.extract_batch"
    depends_on: ["scan_directory"]
    inputs:
      files: "${scan_directory.result.files}"
    
  - id: "quality_check"
    operation: "data_access.quality_check"
    depends_on: ["extract_metadata"]
    inputs:
      metadata: "${extract_metadata.result}"
    
  - id: "update_index"
    operation: "metadata.update_index"
    depends_on: ["quality_check"]
    inputs:
      metadata: "${quality_check.result.valid_metadata}"
    
  - id: "generate_report"
    operation: "reporting.generate_summary"
    depends_on: ["update_index"]
    inputs:
      statistics: "${update_index.result.statistics}"
```

## 时间计划

### 第一阶段（4-6周）：完成第2层
- 周1-2：TaskDispatcher 实现
- 周3-4：数据管理工作流实现
- 周5-6：网络接口集成和测试

### 第二阶段（2-3周）：完善第4层
- 瓦片服务完整实现
- 数据可视化功能

### 第三阶段（1-2周）：完善第1层
- 网络服务完整集成
- 系统整体测试

## 成功标准

### 第2层完成标准
1. ✅ 可以通过HTTP API启动数据管理工作流
2. ✅ 工作流能够异步执行并报告进度
3. ✅ 支持工作流的取消和状态查询
4. ✅ 能够处理大规模数据目录（1000+文件）

### 海洋数据管理功能完成标准
1. ✅ 自动扫描和索引NetCDF文件
2. ✅ 生成完整的元数据数据库
3. ✅ 提供RESTful API查询接口
4. ✅ 生成详细的数据统计报告
5. ✅ 支持增量更新和监控

## 风险和缓解措施

### 主要风险
1. **工作流引擎复杂性**：异步执行和依赖管理复杂
2. **大数据处理性能**：处理大量文件时的内存和性能问题
3. **错误处理**：工作流中某个步骤失败的处理机制

### 缓解措施
1. **分阶段实现**：先实现简单的线性工作流，再添加复杂依赖
2. **流式处理**：使用分批处理避免内存问题
3. **完善测试**：每个阶段都要有充分的单元测试和集成测试

## 下一步行动

1. **立即开始**：TaskDispatcher 的实现
2. **并行进行**：数据管理工作流的设计
3. **持续集成**：确保每个阶段都能与现有模块正确集成 