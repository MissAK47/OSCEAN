#include "common_utils/memory/memory_manager_unified.h"
#include <cstdlib>
#include <cstring>
#include <algorithm>
#include <iostream>

#ifdef _WIN32
#include <malloc.h>
namespace {
    inline void* aligned_alloc_compat(size_t alignment, size_t size) {
        // ç¡®ä¿å¤§å°æ˜¯å¯¹é½å€¼çš„å€æ•°
        size_t aligned_size = (size + alignment - 1) & ~(alignment - 1);
        return _aligned_malloc(aligned_size, alignment);
    }
    
    inline void aligned_free_compat(void* ptr) {
        _aligned_free(ptr);
    }
}
#else
#include <cstdlib>
namespace {
    inline void* aligned_alloc_compat(size_t alignment, size_t size) {
        // ç¡®ä¿å¤§å°æ˜¯å¯¹é½å€¼çš„å€æ•°
        size_t aligned_size = (size + alignment - 1) & ~(alignment - 1);
        return aligned_alloc(alignment, aligned_size);
    }
    
    inline void aligned_free_compat(void* ptr) {
        std::free(ptr);
    }
}
#endif

namespace oscean::common_utils::memory {

// === å†…å­˜ç®¡ç†å¸¸é‡å®šä¹‰ ===
namespace {
    // ğŸ”§ ç»Ÿä¸€çš„å¿«é€Ÿè·¯å¾„é˜ˆå€¼ï¼š256å­—èŠ‚ä»¥ä¸‹çš„å°å¯¹è±¡ä½¿ç”¨å¿«é€Ÿåˆ†é…ï¼Œä¸è®°å½•ç»Ÿè®¡ä¿¡æ¯
    constexpr size_t FAST_PATH_THRESHOLD = 256;
    // ğŸ”§ ä¸­ç­‰å¯¹è±¡é˜ˆå€¼ï¼š2048å­—èŠ‚ä»¥ä¸‹çš„å¯¹è±¡ä½¿ç”¨ç›´æ¥ç³»ç»Ÿåˆ†é…
    constexpr size_t MEDIUM_PATH_THRESHOLD = 2048;
    // ğŸ”§ çº¿ç¨‹ç¼“å­˜é˜ˆå€¼ï¼š1024å­—èŠ‚ä»¥ä¸‹çš„å¯¹è±¡å¯ä»¥å°è¯•çº¿ç¨‹ç¼“å­˜
    constexpr size_t THREAD_CACHE_THRESHOLD = 1024;
}

// === å†…éƒ¨ç»“æ„ä½“å®šä¹‰ ===

struct UnifiedMemoryManager::MemoryPool {
    void* memory;
    size_t size;
    size_t used;
    std::vector<void*> freeBlocks;
    MemoryPoolType poolType;
    std::mutex poolMutex;
    
    MemoryPool(size_t s, MemoryPoolType type) 
        : memory(nullptr), size(s), used(0), poolType(type) {
        memory = aligned_alloc_compat(64, size);
        if (!memory) {
            throw std::bad_alloc();
        }
        std::memset(memory, 0, size);
    }
    
    ~MemoryPool() {
        if (memory) {
            aligned_free_compat(memory);
        }
    }
};

struct UnifiedMemoryManager::ThreadLocalCache {
    static constexpr size_t CACHE_SIZE = 16;
    struct CacheEntry {
        void* ptr;
        size_t size;
        bool inUse;
    };
    
    CacheEntry entries[CACHE_SIZE];
    size_t nextIndex = 0;
    
    ThreadLocalCache() {
        for (auto& entry : entries) {
            entry.ptr = nullptr;
            entry.size = 0;
            entry.inUse = false;
        }
    }
};

struct UnifiedMemoryManager::LargeDataManager {
    std::unordered_map<void*, size_t> largeAllocations;
    std::mutex largeMutex;
    size_t totalLargeMemory = 0;
    
    void* allocateLarge(size_t size) {
        void* ptr = aligned_alloc_compat(64, size);
        if (ptr) {
            std::lock_guard<std::mutex> lock(largeMutex);
            largeAllocations[ptr] = size;
            totalLargeMemory += size;
        }
        return ptr;
    }
    
    bool deallocateLarge(void* ptr) {
        std::lock_guard<std::mutex> lock(largeMutex);
        auto it = largeAllocations.find(ptr);
        if (it != largeAllocations.end()) {
            totalLargeMemory -= it->second;
            largeAllocations.erase(it);
            aligned_free_compat(ptr);
            return true;
        }
        return false;
    }
};

struct UnifiedMemoryManager::AllocationRegistry {
    std::unordered_map<void*, size_t> allocations;
    std::mutex registryMutex;
    
    void recordAllocation(void* ptr, size_t size) {
        if (ptr) {
            std::lock_guard<std::mutex> lock(registryMutex);
            allocations[ptr] = size;
        }
    }
    
    size_t getAllocationSize(void* ptr) {
        if (!ptr) return 0;
        std::lock_guard<std::mutex> lock(registryMutex);
        auto it = allocations.find(ptr);
        return (it != allocations.end()) ? it->second : 0;
    }
    
    bool removeAllocation(void* ptr) {
        if (!ptr) return false;
        std::lock_guard<std::mutex> lock(registryMutex);
        auto it = allocations.find(ptr);
        if (it != allocations.end()) {
            allocations.erase(it);
            return true;
        }
        return false;
    }
};

struct UnifiedMemoryManager::ConcurrentManager {
    std::atomic<size_t> concurrentAllocations{0};
    std::atomic<size_t> maxConcurrentAllocations{0};
    
    void recordAllocation() {
        size_t current = concurrentAllocations.fetch_add(1) + 1;
        size_t maxVal = maxConcurrentAllocations.load();
        while (current > maxVal && 
               !maxConcurrentAllocations.compare_exchange_weak(maxVal, current)) {
            // ç©ºå¾ªç¯ï¼Œç›´åˆ°æˆåŠŸæ›´æ–°æœ€å¤§å€¼
        }
    }
    
    void recordDeallocation() {
        concurrentAllocations.fetch_sub(1);
    }
};

// === çº¿ç¨‹æœ¬åœ°ç¼“å­˜ ===
thread_local UnifiedMemoryManager::ThreadLocalCache UnifiedMemoryManager::tlsCache_;

// === UnifiedMemoryManagerå®ç° ===

UnifiedMemoryManager::UnifiedMemoryManager(const Config& config) 
    : config_(config), allocationStrategy_(AllocationStrategy::BEST_FIT) {
    largeDataManager_ = std::make_unique<LargeDataManager>();
    concurrentManager_ = std::make_unique<ConcurrentManager>();
    allocationRegistry_ = std::make_unique<AllocationRegistry>();
}

UnifiedMemoryManager::~UnifiedMemoryManager() {
    shutdown();
}

bool UnifiedMemoryManager::initialize() {
    try {
        initializePools();
        return true;
    } catch (const std::exception& e) {
        std::cerr << "Failed to initialize memory manager: " << e.what() << std::endl;
        return false;
    }
}

void UnifiedMemoryManager::shutdown() {
    shutdownPools();
}

void* UnifiedMemoryManager::allocate(size_t size, size_t alignment, const std::string& tag) {
    if (size == 0) return nullptr;
    
    // å¦‚æœæŒ‡å®šäº†å¯¹é½è¦æ±‚ï¼Œä¸ä½¿ç”¨å¿«é€Ÿè·¯å¾„
    if (alignment > 8) {
        // ä½¿ç”¨æ ‡å‡†è·¯å¾„ç¡®ä¿æ­£ç¡®å¯¹é½
        void* ptr = allocateFromSystem(size, alignment);
        if (ptr) {
            allocationRegistry_->recordAllocation(ptr, size);
            updateStatistics("aligned_allocation", size);
        }
        return ptr;
    }
    
    // ğŸš€ è¶…å¿«é€Ÿè·¯å¾„ï¼š256å­—èŠ‚ä»¥ä¸‹çš„å°å¯¹è±¡ç›´æ¥åˆ†é…ï¼Œå®Œå…¨è·³è¿‡æ‰€æœ‰ç®¡ç†å¼€é”€
    if (size <= FAST_PATH_THRESHOLD) {
        // ğŸš€ å†…è”æœ€å¿«çš„åˆ†é…è·¯å¾„ï¼Œé¿å…å‡½æ•°è°ƒç”¨å¼€é”€
        size_t useAlignment = (alignment == 0) ? 8 : alignment;
#ifdef _WIN32
        return _aligned_malloc(size, useAlignment);  // ç›´æ¥è°ƒç”¨æœ€å¿«çš„Windows API
#else
        return aligned_alloc(useAlignment, size);    // ç›´æ¥è°ƒç”¨æœ€å¿«çš„Linux API
#endif
    }
    
    // ğŸ”§ æ ‡å‡†è·¯å¾„ï¼šéœ€è¦å®Œæ•´çš„ç®¡ç†å’Œç»Ÿè®¡ï¼ˆä»…ç”¨äºå¤§å¯¹è±¡ï¼‰
    concurrentManager_->recordAllocation();
    
    void* ptr = nullptr;
    
    // ğŸŠâ€â™‚ï¸ ä¸­ç­‰å¯¹è±¡ä¼˜å…ˆå°è¯•æ± åˆ†é…
    if (size <= MEDIUM_PATH_THRESHOLD) {
        ptr = allocateFromPool(size, alignment);
        if (ptr) {
            allocationRegistry_->recordAllocation(ptr, size);
            updateStatistics("pool_allocation", size);
            return ptr;
        }
        
        // æ± åˆ†é…å¤±è´¥ï¼Œé™çº§åˆ°ç³»ç»Ÿåˆ†é…
        size_t useAlignment = (alignment == 0 || alignment > 64) ? 64 : alignment;
        ptr = aligned_alloc_compat(useAlignment, size);
        if (ptr) {
            allocationRegistry_->recordAllocation(ptr, size);
            updateStatistics("system_allocation", size);
        }
        return ptr;
    }
    
    // å¤§å¯¹è±¡è·¯å¾„
    ptr = allocateFromSystem(size, alignment);
    if (ptr) {
        allocationRegistry_->recordAllocation(ptr, size);
        updateStatistics("system_allocation", size);
    }
    
    return ptr;
}

void UnifiedMemoryManager::deallocate(void* ptr) {
    if (!ptr) return;
    
    // ğŸš€ è¶…å¿«é€Ÿè·¯å¾„æ£€æµ‹ï¼šå¦‚æœä¸åœ¨æ³¨å†Œè¡¨ä¸­ï¼Œå¾ˆå¯èƒ½æ˜¯å¿«é€Ÿè·¯å¾„åˆ†é…çš„å°å¯¹è±¡
    size_t allocSize = 0;
    {
        std::lock_guard<std::mutex> lock(allocationRegistry_->registryMutex);
        auto it = allocationRegistry_->allocations.find(ptr);
        if (it != allocationRegistry_->allocations.end()) {
            allocSize = it->second;
            allocationRegistry_->allocations.erase(it);
        }
    }
    
    if (allocSize > 0) {
        // ğŸ”§ æ ‡å‡†è·¯å¾„ï¼šæœ‰è®°å½•çš„å¯¹è±¡ï¼Œå®Œæ•´é‡Šæ”¾æµç¨‹
        concurrentManager_->recordDeallocation();
        
        // å°è¯•æ”¾å…¥çº¿ç¨‹ç¼“å­˜
        if (allocSize <= THREAD_CACHE_THRESHOLD && deallocateToThreadCache(ptr, allocSize)) {
            updateStatistics("thread_cache_deallocation", allocSize);
            return;
        }
        
        // è¿”å›æ± ä¸­
        if (deallocateToPool(ptr, allocSize)) {
            updateStatistics("pool_deallocation", allocSize);
            return;
        }
        
        // ç³»ç»Ÿé‡Šæ”¾
        aligned_free_compat(ptr);
        updateStatistics("system_deallocation", allocSize);
        return;
    }
    
    // ğŸš€ å¿«é€Ÿè·¯å¾„é‡Šæ”¾ï¼šå¯èƒ½æ˜¯256å­—èŠ‚ä»¥ä¸‹çš„å°å¯¹è±¡ï¼Œç›´æ¥é‡Šæ”¾
    // ç®€å•çš„æœ‰æ•ˆæ€§æ£€æŸ¥
    uintptr_t addr = reinterpret_cast<uintptr_t>(ptr);
    if (addr < 0x1000) {  // åªæ’é™¤æ˜æ˜¾æ— æ•ˆçš„åœ°å€
        return;
    }
    
    // ğŸš€ ç›´æ¥é‡Šæ”¾ï¼Œæœ€å°åŒ–å¼€é”€
#ifdef _WIN32
    _aligned_free(ptr);  // ç›´æ¥è°ƒç”¨æœ€å¿«çš„Windows API
#else
    std::free(ptr);      // ç›´æ¥è°ƒç”¨æœ€å¿«çš„Linux API
#endif
}

void* UnifiedMemoryManager::reallocate(void* ptr, size_t newSize, const std::string& tag) {
    if (!ptr) {
        return allocate(newSize, 0, tag);
    }
    
    if (newSize == 0) {
        deallocate(ptr);
        return nullptr;
    }
    
    // ğŸ”§ ä¿®å¤ï¼šè·å–åŸå§‹åˆ†é…å¤§å°
    size_t oldSize = allocationRegistry_->getAllocationSize(ptr);
    if (oldSize == 0) {
        // æ£€æŸ¥æ˜¯å¦ä¸ºå¤§å¯¹è±¡
        std::lock_guard<std::mutex> lock(largeDataManager_->largeMutex);
        auto it = largeDataManager_->largeAllocations.find(ptr);
        if (it != largeDataManager_->largeAllocations.end()) {
            oldSize = it->second;
        }
    }
    
    // åˆ†é…æ–°å†…å­˜
    void* newPtr = allocate(newSize, 0, tag);
    if (newPtr && ptr && oldSize > 0) {
        // ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ‹·è´å¤§å° - min(oldSize, newSize)
        size_t copySize = std::min(oldSize, newSize);
        std::memcpy(newPtr, ptr, copySize);
        deallocate(ptr);
    } else if (!newPtr) {
        // åˆ†é…å¤±è´¥ï¼Œä¸é‡Šæ”¾åŸå†…å­˜
        return nullptr;
    }
    
    return newPtr;
}

bool UnifiedMemoryManager::isManaged(void* ptr) const {
    // æ£€æŸ¥æ˜¯å¦åœ¨ä»»ä½•æ± ä¸­
    std::shared_lock<std::shared_mutex> lock(poolMutex_);
    for (const auto& pool : pools_) {
        char* poolStart = static_cast<char*>(pool->memory);
        char* poolEnd = poolStart + pool->size;
        char* ptrAddr = static_cast<char*>(ptr);
        
        if (ptrAddr >= poolStart && ptrAddr < poolEnd) {
            return true;
        }
    }
    
    // æ£€æŸ¥å¤§å¯¹è±¡
    std::lock_guard<std::mutex> largeLock(largeDataManager_->largeMutex);
    return largeDataManager_->largeAllocations.find(ptr) != 
           largeDataManager_->largeAllocations.end();
}

size_t UnifiedMemoryManager::getPoolSize() const {
    std::shared_lock<std::shared_mutex> lock(poolMutex_);
    size_t totalSize = 0;
    for (const auto& pool : pools_) {
        totalSize += pool->size;
    }
    return totalSize + largeDataManager_->totalLargeMemory;
}

size_t UnifiedMemoryManager::getAvailableMemory() const {
    std::lock_guard<std::mutex> lock(statsMutex_);
    size_t totalUsed = stats_.currentUsed;
    size_t maxMemory = config_.maxTotalMemoryMB * 1024 * 1024;
    return maxMemory > totalUsed ? maxMemory - totalUsed : 0;
}

MemoryUsageStats UnifiedMemoryManager::getUsageStats() const {
    return stats_;
}

void UnifiedMemoryManager::setAllocationStrategy(AllocationStrategy strategy) {
    allocationStrategy_ = strategy;
}

void UnifiedMemoryManager::setMemoryUsageCallback(
    std::function<void(const MemoryUsageStats&)> callback) {
    memoryUsageCallback_ = callback;
}

// === æµå¼ç¼“å†²åŒºå®ç° ===

UnifiedMemoryManager::StreamingBuffer::StreamingBuffer(
    UnifiedMemoryManager& manager, size_t maxSize)
    : manager_(manager), maxSize_(maxSize), currentSize_(0) {
    buffer_ = manager_.allocate(maxSize);
}

UnifiedMemoryManager::StreamingBuffer::~StreamingBuffer() {
    if (buffer_) {
        manager_.deallocate(buffer_);
    }
}

void* UnifiedMemoryManager::StreamingBuffer::getWriteBuffer(size_t size) {
    std::lock_guard<std::mutex> lock(bufferMutex_);
    if (size > maxSize_) return nullptr;
    
    currentSize_ = size;
    return buffer_;
}

void UnifiedMemoryManager::StreamingBuffer::commitBuffer(size_t actualSize) {
    std::lock_guard<std::mutex> lock(bufferMutex_);
    currentSize_ = std::min(actualSize, maxSize_);
}

const void* UnifiedMemoryManager::StreamingBuffer::getReadBuffer() const {
    return buffer_;
}

size_t UnifiedMemoryManager::StreamingBuffer::getBufferSize() const noexcept {
    return currentSize_;
}

void UnifiedMemoryManager::StreamingBuffer::resetBuffer() noexcept {
    std::lock_guard<std::mutex> lock(bufferMutex_);
    currentSize_ = 0;
}

std::unique_ptr<UnifiedMemoryManager::StreamingBuffer> 
UnifiedMemoryManager::createStreamingBuffer(size_t maxSize) {
    return std::make_unique<StreamingBuffer>(*this, maxSize);
}

// === å¹¶å‘åˆ†é…å™¨å®ç° ===

UnifiedMemoryManager::ConcurrentAllocator::ConcurrentAllocator(
    UnifiedMemoryManager& manager) : manager_(manager) {}

UnifiedMemoryManager::ConcurrentAllocator::~ConcurrentAllocator() = default;

void* UnifiedMemoryManager::ConcurrentAllocator::allocate(size_t size) noexcept {
    try {
        return manager_.allocate(size);
    } catch (...) {
        return nullptr;
    }
}

void UnifiedMemoryManager::ConcurrentAllocator::deallocate(void* ptr) noexcept {
    manager_.deallocate(ptr);
}

MemoryUsageStats UnifiedMemoryManager::ConcurrentAllocator::getThreadStats() const noexcept {
    std::lock_guard<std::mutex> lock(statsMutex_);
    return threadStats_;
}

std::unique_ptr<UnifiedMemoryManager::ConcurrentAllocator> 
UnifiedMemoryManager::createConcurrentAllocator() {
    return std::make_unique<ConcurrentAllocator>(*this);
}

// === å†…éƒ¨å®ç°æ–¹æ³• ===

void UnifiedMemoryManager::initializePools() {
    std::lock_guard<std::shared_mutex> lock(poolMutex_);
    
    // åˆ›å»ºåŸºç¡€æ± 
    size_t poolSize = config_.chunkSizeMB * 1024 * 1024;
    pools_.push_back(std::make_unique<MemoryPool>(poolSize, MemoryPoolType::GENERAL_PURPOSE));
    
    // æ ¹æ®é…ç½®åˆ›å»ºå…¶ä»–æ± 
    if (config_.enableSIMDOptimization) {
        pools_.push_back(std::make_unique<MemoryPool>(poolSize / 2, MemoryPoolType::SIMD_ALIGNED));
    }
    
    if (config_.enableLargeDataSupport) {
        pools_.push_back(std::make_unique<MemoryPool>(poolSize * 2, MemoryPoolType::LARGE_OBJECTS));
    }
}

void UnifiedMemoryManager::shutdownPools() {
    std::lock_guard<std::shared_mutex> lock(poolMutex_);
    pools_.clear();
}

void* UnifiedMemoryManager::allocateFromPool(size_t size, size_t alignment) {
    std::shared_lock<std::shared_mutex> lock(poolMutex_);
    
    for (auto& pool : pools_) {
        std::lock_guard<std::mutex> poolLock(pool->poolMutex);
        
        // ğŸš€ è¶…å¿«é€ŸæŸ¥æ‰¾ï¼šä¼˜å…ˆæŸ¥çœ‹freeBlocksä¸­çš„å¯ç”¨å†…å­˜å—
        for (auto it = pool->freeBlocks.begin(); it != pool->freeBlocks.end(); ++it) {
            void* freePtr = *it;
            
            // ç®€å•å¯¹é½æ£€æŸ¥
            uintptr_t addr = reinterpret_cast<uintptr_t>(freePtr);
            if (alignment > 0 && (addr % alignment) != 0) {
                continue;  // å¯¹é½ä¸æ»¡è¶³ï¼Œç»§ç»­æŸ¥æ‰¾
            }
            
            // æ‰¾åˆ°å¯ç”¨å—ï¼Œç«‹å³è¿”å›
            pool->freeBlocks.erase(it);
            return freePtr;
        }
        
        // ğŸš€ å¦‚æœfreeBlocksä¸­æ²¡æœ‰ï¼Œä»æ± å°¾åˆ†é…æ–°å†…å­˜
        if (pool->used + size <= pool->size) {
            char* basePtr = static_cast<char*>(pool->memory) + pool->used;
            
            // å¤„ç†å¯¹é½
            uintptr_t addr = reinterpret_cast<uintptr_t>(basePtr);
            size_t alignedSize = size;
            
            if (alignment > 0) {
                uintptr_t aligned = (addr + alignment - 1) & ~(alignment - 1);
                alignedSize = size + (aligned - addr);
                basePtr = reinterpret_cast<char*>(aligned);
            }
            
            if (pool->used + alignedSize <= pool->size) {
                pool->used += alignedSize;
                return basePtr;
            }
        }
    }
    
    return nullptr;  // æ‰€æœ‰æ± éƒ½æ»¡äº†
}

void* UnifiedMemoryManager::allocateFromSystem(size_t size, size_t alignment) {
    // ğŸ”§ ä¿®å¤ï¼šç¡®ä¿å¯¹é½å‚æ•°æ˜¯2çš„å¹‚æ¬¡ä¸”è‡³å°‘ä¸º8
    if (alignment == 0) {
        alignment = config_.alignmentSize; // ä½¿ç”¨é…ç½®çš„é»˜è®¤å¯¹é½
    }
    
    // ğŸ”§ ä¿®å¤ï¼šåªæœ‰å½“å¯¹é½å€¼ä¸æ˜¯2çš„å¹‚æ¬¡æ—¶æ‰è°ƒæ•´
    if ((alignment & (alignment - 1)) != 0) {
        // å‘ä¸Šè°ƒæ•´åˆ°æœ€è¿‘çš„2çš„å¹‚æ¬¡
        size_t powerOfTwo = 1;
        while (powerOfTwo < alignment) {
            powerOfTwo <<= 1;
        }
        alignment = powerOfTwo;
    }
    
    // æœ€å°å¯¹é½ä¸º8å­—èŠ‚
    if (alignment < 8) {
        alignment = 8;
    }
    
    return aligned_alloc_compat(alignment, size);
}

void* UnifiedMemoryManager::allocateFromThreadCache(size_t size) {
    // ç®€åŒ–çš„çº¿ç¨‹ç¼“å­˜å®ç°
    for (auto& entry : tlsCache_.entries) {
        if (!entry.inUse && entry.size >= size) {
            entry.inUse = true;
            return entry.ptr;
        }
    }
    return nullptr;
}

bool UnifiedMemoryManager::deallocateToPool(void* ptr, size_t size) {
    // ğŸ”§ å®ç°çœŸæ­£çš„æ± é‡Šæ”¾åŠŸèƒ½è€Œä¸æ˜¯ç®€åŒ–å®ç°
    if (!ptr || size == 0) return false;
    
    std::shared_lock<std::shared_mutex> lock(poolMutex_);
    
    for (auto& pool : pools_) {
        std::lock_guard<std::mutex> poolLock(pool->poolMutex);
        
        // æ£€æŸ¥æŒ‡é’ˆæ˜¯å¦åœ¨æ­¤æ± çš„èŒƒå›´å†…
        char* poolStart = static_cast<char*>(pool->memory);
        char* poolEnd = poolStart + pool->size;
        char* ptrAddr = static_cast<char*>(ptr);
        
        if (ptrAddr >= poolStart && ptrAddr < poolEnd) {
            // å°†é‡Šæ”¾çš„å†…å­˜å—æ·»åŠ åˆ°è‡ªç”±å—åˆ—è¡¨
            pool->freeBlocks.push_back(ptr);
            
            // ğŸ”§ å¯é€‰ä¼˜åŒ–ï¼šå¦‚æœé‡Šæ”¾çš„æ˜¯æ± æœ«å°¾çš„å†…å­˜ï¼Œç›´æ¥å‡å°‘used
            if (ptrAddr + size == poolStart + pool->used) {
                pool->used -= size;
                // ä»freeBlocksä¸­ç§»é™¤åˆšæ·»åŠ çš„æŒ‡é’ˆ
                if (!pool->freeBlocks.empty()) {
                    pool->freeBlocks.pop_back();
                }
            }
            
            return true;  // æˆåŠŸé‡Šæ”¾åˆ°æ± 
        }
    }
    
    return false;  // ä¸åœ¨ä»»ä½•æ± ä¸­
}

bool UnifiedMemoryManager::deallocateToThreadCache(void* ptr, size_t size) {
    // ç®€åŒ–çš„çº¿ç¨‹ç¼“å­˜å®ç°
    for (auto& entry : tlsCache_.entries) {
        if (entry.ptr == ptr) {
            entry.inUse = false;
            return true;
        }
    }
    
    // å°è¯•æ·»åŠ åˆ°ç©ºé—²æ§½
    for (auto& entry : tlsCache_.entries) {
        if (entry.ptr == nullptr) {
            entry.ptr = ptr;
            entry.size = size;
            entry.inUse = false;
            return true;
        }
    }
    
    return false;
}

void UnifiedMemoryManager::updateStatistics(const std::string& operation, size_t size) {
    std::lock_guard<std::mutex> lock(statsMutex_);  // æ·»åŠ mutexä¿æŠ¤
    
    // ğŸ”§ ä¿®å¤ï¼šå…ˆæ£€æŸ¥deallocationï¼Œé¿å…ä¸allocationå†²çª
    if (operation.find("deallocation") != std::string::npos) {
        stats_.deallocationCount++;
        if (size > 0 && stats_.currentUsed >= size) {
            stats_.currentUsed -= size;
        }
    } else if (operation.find("allocation") != std::string::npos) {
        stats_.allocationCount++;
        stats_.currentUsed += size;
        stats_.totalAllocated += size;
        
        // æ›´æ–°å³°å€¼ä½¿ç”¨é‡
        if (stats_.currentUsed > stats_.peakUsage) {
            stats_.peakUsage = stats_.currentUsed;
        }
    }
    
    // è°ƒç”¨ç”¨æˆ·å›è°ƒ
    if (memoryUsageCallback_) {
        memoryUsageCallback_(stats_);
    }
}

void UnifiedMemoryManager::checkMemoryPressure() {
    std::lock_guard<std::mutex> lock(statsMutex_);
    size_t currentUsageMB = stats_.currentUsed / (1024 * 1024);
    if (currentUsageMB > config_.memoryPressureThresholdMB) {
        if (memoryPressureCallback_) {
            memoryPressureCallback_(MemoryPressureLevel::HIGH);
        }
    }
}

UnifiedMemoryManager::MemoryPressureLevel 
UnifiedMemoryManager::getMemoryPressure() const noexcept {
    std::lock_guard<std::mutex> lock(statsMutex_);
    size_t currentUsageMB = stats_.currentUsed / (1024 * 1024);
    size_t threshold = config_.memoryPressureThresholdMB;
    
    if (currentUsageMB < threshold / 2) {
        return MemoryPressureLevel::LOW;
    } else if (currentUsageMB < threshold * 3 / 4) {
        return MemoryPressureLevel::MEDIUM;
    } else if (currentUsageMB < threshold) {
        return MemoryPressureLevel::HIGH;
    } else {
        return MemoryPressureLevel::CRITICAL;
    }
}

void UnifiedMemoryManager::setMemoryPressureCallback(
    std::function<void(MemoryPressureLevel)> callback) {
    memoryPressureCallback_ = callback;
}

void UnifiedMemoryManager::triggerGarbageCollection() {
    // ğŸ”§ ä¿®å¤ï¼šå®‰å…¨çš„åƒåœ¾å›æ”¶å®ç°ï¼Œé¿å…åŒé‡é‡Šæ”¾
    for (auto& entry : tlsCache_.entries) {
        if (!entry.inUse && entry.ptr) {
            // ğŸ”§ æ£€æŸ¥æŒ‡é’ˆæ˜¯å¦åœ¨åˆ†é…æ³¨å†Œè¡¨ä¸­ï¼Œé¿å…åŒé‡é‡Šæ”¾
            size_t allocSize = allocationRegistry_->getAllocationSize(entry.ptr);
            if (allocSize > 0) {
                // ä»æ³¨å†Œè¡¨ä¸­ç§»é™¤
                allocationRegistry_->removeAllocation(entry.ptr);
                // é‡Šæ”¾å†…å­˜
                aligned_free_compat(entry.ptr);
                // æ›´æ–°ç»Ÿè®¡
                updateStatistics("garbage_collection_deallocation", allocSize);
            }
            // æ¸…ç†ç¼“å­˜æ¡ç›®
            entry.ptr = nullptr;
            entry.size = 0;
        }
    }
}

void UnifiedMemoryManager::optimizeMemoryLayout() {
    // ç®€åŒ–å®ç°ï¼šè§¦å‘åƒåœ¾å›æ”¶
    triggerGarbageCollection();
}

void UnifiedMemoryManager::dumpAllocations() const {
    std::cout << "=== Memory Manager Statistics ===" << std::endl;
    
    // ğŸ”§ æ‰‹åŠ¨è¾“å‡ºç»Ÿè®¡ä¿¡æ¯ï¼Œå› ä¸ºtoString()æ–¹æ³•ä¸å­˜åœ¨
    std::lock_guard<std::mutex> lock(statsMutex_);
    std::cout << "Total Allocated: " << stats_.totalAllocated << " bytes" << std::endl;
    std::cout << "Currently Used: " << stats_.currentUsed << " bytes" << std::endl;
    std::cout << "Peak Usage: " << stats_.peakUsage << " bytes" << std::endl;
    std::cout << "Allocation Count: " << stats_.allocationCount << std::endl;
    std::cout << "Deallocation Count: " << stats_.deallocationCount << std::endl;
    std::cout << "Fragmentation Ratio: " << stats_.fragmentationRatio << std::endl;
    
    std::cout << "\n=== Pool Information ===" << std::endl;
    std::shared_lock<std::shared_mutex> poolLock(poolMutex_);
    for (size_t i = 0; i < pools_.size(); ++i) {
        const auto& pool = pools_[i];
        std::lock_guard<std::mutex> poolMutexLock(pool->poolMutex);
        std::cout << "Pool " << i << ": " << pool->used << "/" << pool->size 
                  << " bytes used" << std::endl;
    }
    
    std::cout << "\n=== Large Objects ===" << std::endl;
    std::lock_guard<std::mutex> largeLock(largeDataManager_->largeMutex);
    std::cout << "Large objects count: " << largeDataManager_->largeAllocations.size() << std::endl;
    std::cout << "Total large memory: " << largeDataManager_->totalLargeMemory << " bytes" << std::endl;
}

bool UnifiedMemoryManager::performHealthCheck() const {
    // ğŸ”§ ç®€åŒ–å¥åº·æ£€æŸ¥å®ç°
    std::lock_guard<std::mutex> lock(statsMutex_);
    bool memoryHealthy = (stats_.fragmentationRatio < 0.5) && 
                        (getMemoryPressure() != MemoryPressureLevel::CRITICAL);
    return memoryHealthy;
}

// === ç±»å‹åŒ–åˆ†é…æ”¯æŒå®ç° ===

void* UnifiedMemoryManager::allocateWithTraits(size_t size, size_t alignment, 
                                              MemoryPoolType poolType, const MemoryTraits& traits) {
    // ç®€åŒ–å®ç°ï¼šä½¿ç”¨æ ‡å‡†åˆ†é…ï¼Œè€ƒè™‘traitsä¸­çš„ç‰¹æ®Šè¦æ±‚
    if (traits.zeroInitialized) {
        void* ptr = allocate(size, alignment);
        if (ptr) {
            std::memset(ptr, 0, size);
        }
        return ptr;
    }
    
    return allocate(size, alignment, traits.memoryTag);
}

void UnifiedMemoryManager::deallocateWithTraits(void* ptr, size_t size, 
                                               MemoryPoolType poolType, const MemoryTraits& traits) {
    deallocate(ptr);
}

// === æ‰¹é‡åˆ†é…æ”¯æŒå®ç° ===

std::vector<void*> UnifiedMemoryManager::allocateBatch(const std::vector<size_t>& sizes) {
    std::vector<void*> results;
    results.reserve(sizes.size());
    
    if (sizes.empty()) {
        return results;
    }
    
    // ğŸš€ æç®€åŒ–æ‰¹é‡åˆ†é…ï¼šå¯¹äºå°å¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨æœ€å¿«è·¯å¾„ï¼Œå®Œå…¨è·³è¿‡è®°å½•
    for (size_t size : sizes) {
        if (size <= FAST_PATH_THRESHOLD) {
            // ğŸš€ å¿«é€Ÿè·¯å¾„ï¼šç›´æ¥åˆ†é…ï¼Œæ— ä»»ä½•è®°å½•æˆ–ç»Ÿè®¡å¼€é”€
#ifdef _WIN32
            results.push_back(_aligned_malloc(size, 8));
#else
            results.push_back(aligned_alloc(8, size));
#endif
        } else {
            // å¤§å¯¹è±¡ï¼šä½¿ç”¨æ ‡å‡†è·¯å¾„
            results.push_back(allocate(size));
        }
    }
    
    return results;
}

void UnifiedMemoryManager::deallocateBatch(const std::vector<std::pair<void*, size_t>>& allocations) {
    if (allocations.empty()) return;
    
    size_t fastPathCount = 0;
    size_t trackedDeallocations = 0;
    size_t totalDeallocatedTracked = 0;
    
    for (const auto& alloc : allocations) {
        void* ptr = alloc.first;
        size_t size = alloc.second;
        
        if (!ptr) continue;
        
        if (size <= FAST_PATH_THRESHOLD) {
            // ğŸ”§ å¿«é€Ÿè·¯å¾„å¯¹è±¡ï¼šç›´æ¥é‡Šæ”¾ï¼Œä¸åšè®°å½•ï¼ˆä¸åˆ†é…æ—¶ä¸€è‡´ï¼‰
            aligned_free_compat(ptr);
            fastPathCount++;
        } else {
            // ğŸ”§ è·Ÿè¸ªå¯¹è±¡ï¼šä½¿ç”¨æ ‡å‡†é‡Šæ”¾è·¯å¾„
            deallocate(ptr);
            trackedDeallocations++;
            totalDeallocatedTracked += size;
        }
    }
    
    // ğŸ”§ æ‰¹é‡ç»Ÿè®¡æ›´æ–°ï¼ˆåªç»Ÿè®¡é‡Šæ”¾è®¡æ•°ï¼Œä¸é‡å¤ç»Ÿè®¡å†…å­˜ï¼‰
    if (fastPathCount > 0) {
        std::lock_guard<std::mutex> statsLock(statsMutex_);
        // åªå¢åŠ é‡Šæ”¾è®¡æ•°ï¼Œä¸è°ƒæ•´å†…å­˜ä½¿ç”¨é‡ï¼ˆå› ä¸ºå¿«é€Ÿè·¯å¾„åˆ†é…æ—¶ä¹Ÿæ²¡ç»Ÿè®¡ï¼‰
        stats_.deallocationCount += fastPathCount;
    }
    
    // æ³¨æ„ï¼štrackedDeallocations çš„ç»Ÿè®¡å·²ç»åœ¨ deallocate() è°ƒç”¨ä¸­å¤„ç†äº†
}

// === é«˜çº§åŠŸèƒ½å®ç° ===

void* UnifiedMemoryManager::allocateOnNUMANode(size_t size, int node) {
    // ç®€åŒ–å®ç°ï¼šå¿½ç•¥NUMAèŠ‚ç‚¹ï¼Œä½¿ç”¨æ ‡å‡†åˆ†é…
    return allocate(size);
}

void* UnifiedMemoryManager::allocateSIMDAligned(size_t size, size_t vectorWidth) {
    // ç¡®ä¿vectorWidthæ˜¯æœ‰æ•ˆçš„å¯¹é½å€¼ï¼ˆ2çš„å¹‚æ¬¡ï¼‰
    if (vectorWidth == 0 || (vectorWidth & (vectorWidth - 1)) != 0) {
        vectorWidth = 32; // é»˜è®¤AVX2å¯¹é½
    }
    
    // å¼ºåˆ¶ä½¿ç”¨ç³»ç»Ÿåˆ†é…ç¡®ä¿æ­£ç¡®å¯¹é½
    return allocateFromSystem(size, vectorWidth);
}

void UnifiedMemoryManager::prefetchMemory(void* ptr, size_t size) {
    // ç®€åŒ–å®ç°ï¼šåœ¨å®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨__builtin_prefetch
    (void)ptr;
    (void)size;
}

void UnifiedMemoryManager::preAllocatePool(MemoryPoolType poolType, size_t sizeMB) {
    // ç®€åŒ–å®ç°ï¼šåˆ›å»ºæŒ‡å®šå¤§å°çš„æ± 
    std::lock_guard<std::shared_mutex> lock(poolMutex_);
    size_t poolSize = sizeMB * 1024 * 1024;
    pools_.push_back(std::make_unique<MemoryPool>(poolSize, poolType));
}

// === è¯¦ç»†ç»Ÿè®¡å®ç° ===

UnifiedMemoryManager::DetailedStats UnifiedMemoryManager::getDetailedStatistics() const {
    DetailedStats detailedStats;
    detailedStats.overall = stats_;
    
    // æ”¶é›†å„æ± çš„ç»Ÿè®¡ä¿¡æ¯
    std::shared_lock<std::shared_mutex> lock(poolMutex_);
    for (const auto& pool : pools_) {
        std::lock_guard<std::mutex> poolLock(pool->poolMutex);
        
        MemoryUsageStats poolStats;
        poolStats.totalAllocated = pool->size;
        poolStats.currentUsed = pool->used;
        poolStats.peakUsage = pool->used; // ç®€åŒ–
        
        detailedStats.poolStats[pool->poolType] = poolStats;
    }
    
    // çº¿ç¨‹æœ¬åœ°ç¼“å­˜ç»Ÿè®¡
    detailedStats.threadLocalCacheHits = 0;
    detailedStats.threadLocalCacheMisses = 0;
    
    // æ€§èƒ½ç»Ÿè®¡
    detailedStats.averageAllocationTime = 0.001; // 1ms ç®€åŒ–å€¼
    detailedStats.averageDeallocationTime = 0.0005; // 0.5ms ç®€åŒ–å€¼
    
    return detailedStats;
}

UnifiedMemoryManager::MemoryPool* UnifiedMemoryManager::getOrCreatePool(MemoryPoolType poolType) {
    std::shared_lock<std::shared_mutex> lock(poolMutex_);
    
    // æŸ¥æ‰¾ç°æœ‰æ± 
    for (const auto& pool : pools_) {
        if (pool->poolType == poolType) {
            return pool.get();
        }
    }
    
    // å¦‚æœæ²¡æ‰¾åˆ°ï¼Œåˆ›å»ºæ–°æ± 
    lock.unlock();
    std::lock_guard<std::shared_mutex> writeLock(poolMutex_);
    
    // åŒé‡æ£€æŸ¥
    for (const auto& pool : pools_) {
        if (pool->poolType == poolType) {
            return pool.get();
        }
    }
    
    // åˆ›å»ºæ–°æ± 
    size_t poolSize = config_.chunkSizeMB * 1024 * 1024;
    pools_.push_back(std::make_unique<MemoryPool>(poolSize, poolType));
    return pools_.back().get();
}

// === å·¥å‚æ–¹æ³•å®ç° ===

std::unique_ptr<UnifiedMemoryManager> UnifiedMemoryManager::createForEnvironment(const std::string& environment) {
    Config config;
    
    if (environment == "hpc") {
        config.chunkSizeMB = 2048;  // 2GB for HPC
        config.largeDataThresholdMB = 128;
        config.enableThreadLocalCache = true;
        config.enableSIMDOptimization = true;
    } else if (environment == "production") {
        config.chunkSizeMB = 1024;  // 1GB for production
        config.largeDataThresholdMB = 64;
        config.enableThreadLocalCache = true;
        config.enableSIMDOptimization = false;
    } else if (environment == "testing") {
        config.chunkSizeMB = 128;   // 128MB for testing
        config.largeDataThresholdMB = 16;
        config.enableThreadLocalCache = false;
        config.enableSIMDOptimization = false;
    } else { // development
        config.chunkSizeMB = 256;   // 256MB for development
        config.largeDataThresholdMB = 32;
        config.enableThreadLocalCache = true;
        config.enableSIMDOptimization = false;
    }
    
    auto manager = std::make_unique<UnifiedMemoryManager>(config);
    if (!manager->initialize()) {
        throw std::runtime_error("Failed to initialize memory manager for " + environment);
    }
    
    return manager;
}

UnifiedMemoryManager::Statistics UnifiedMemoryManager::getStatistics() const {
    std::lock_guard<std::mutex> lock(statsMutex_);
    
    Statistics stats;
    stats.currentlyAllocated = stats_.currentUsed;
    stats.maxAllocation = stats_.peakUsage;
    stats.totalAllocations = stats_.allocationCount;
    stats.totalDeallocations = stats_.deallocationCount;
    
    if (stats.totalAllocations > 0) {
        stats.averageAllocationSize = static_cast<double>(stats.currentlyAllocated) / stats.totalAllocations;
    }
    
    return stats;
}

} // namespace oscean::common_utils::memory 
