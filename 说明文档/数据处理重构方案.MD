# Data Access 模块完全重构执行方案

## 📋 1. 重构概览

### 1.1 核心目标
- **彻底消除代码重复**：删除 ~3,200 行重复代码，主要集中在缓存、异步处理、时间转换等方面。
- **统一基础设施**：全面采用并集成 `common_utils` 提供的成熟组件（缓存、异步、时间、日志、错误处理等）。
- **架构现代化**：实现一个接口清晰、职责单一、可扩展、高内聚、低耦合的现代化数据访问架构。
- **提升可维护性与可测试性**：通过清晰的模块划分和接口设计，降低维护成本，提高单元测试覆盖率。
- **流式处理能力**：为处理大规模数据集（TB级）提供高效、内存可控的流式读取能力。
- **性能优化**：显著提升缓存命中率、内存利用率和并发处理能力。

### 1.2 关键收益预期
- **缓存命中率**：从 45-60% 提升至 80-85%。
- **内存利用率**：从 60-70% 提升至 85-90%。
- **代码行数**：净减少约 1,400 行。
- **维护复杂度**：预计降低 60% 以上。
- **系统稳定性与扩展性**：显著增强。

## 🗂️ 2. 最终目录结构与文件清单

### 2.1 目标目录结构 (`core_services_impl/data_access_service/`)
```
core_services_impl/data_access_service/
├── include/core_services/data_access/  # 公共接口和核心类型 (对外暴露)
│   ├── i_data_access_service.h         # (保持不变) 服务主接口
│   ├── common_types.h                  # (新增/重构) 模块特定的公共类型定义 (如 DataChunkKey, FileMetadata, BoundingBox, TimeRange, DimensionDefinition 等)
│   ├── error_codes.h                   # (新增) 模块特定的错误码定义
│   └── api/                              # (新增) 新的核心接口定义
│       ├── i_data_source.h
│       ├── i_metadata_provider.h
│       ├── i_data_provider.h
│       ├── i_streaming_data_provider.h
│       ├── data_access_requests.h      # (GridReadRequest, FeatureReadRequest, etc.)
│       ├── data_access_responses.h     # (GridData, FeatureCollection, DataChunk etc.)
│       └── streaming_types.h           # (StreamingOptions, BackpressureControl etc.)
│
└── src/                                  # 实现代码 (内部细节)
    ├── data_access_service_impl.h        # (重命名/重构自 raw_data_access_service_impl.h)
    ├── data_access_service_impl.cpp      # (重命名/重构自 raw_data_access_service_impl.cpp)
    ├── cache/                              # (新增) 统一缓存实现
    │   ├── unified_data_access_cache.h
    │   ├── unified_data_access_cache.cpp
    │   └── i_cacheable_value.h           # (新增) 可缓存对象接口
    ├── async/                              # (新增) 统一异步执行器
    │   ├── unified_async_executor.h
    │   └── unified_async_executor.cpp
    ├── time/                               # (新增) 时间处理相关
    │   ├── cf_time_extractor.h
    │   └── cf_time_extractor.cpp
    ├── readers/                            # (重构) 读取器相关实现
    │   ├── core/                           # (新增) 读取器核心组件
    │   │   ├── unified_data_reader.h     # 抽象基类
    │   │   ├── unified_data_reader.cpp
    │   │   ├── reader_registry.h
    │   │   ├── reader_registry.cpp
    │   │   ├── i_format_detector.h       # 格式探测器接口
    │   │   ├── format_detector_impl.h    # 具体格式探测器实现
    │   │   └── format_detector_impl.cpp
    │   ├── gdal/                           # (重构) GDAL 读取器实现
    │   │   ├── gdal_unified_reader.h       # 继承 UnifiedDataReader
    │   │   ├── gdal_unified_reader.cpp
    │   │   └── internal/                   # GDAL内部辅助组件 (保持封装, 内容重构)
    │   │       ├── gdal_dataset_handler.h/cpp
    │   │       ├── gdal_metadata_extractor.h/cpp (各类元数据提取器)
    │   │       ├── gdal_raster_io.h/cpp
    │   │       ├── gdal_vector_io.h/cpp
    │   │       └── utils/ (gdal_common_utils, gdal_type_conversion etc.)
    │   └── netcdf/                         # (重构) NetCDF 读取器实现
    │       ├── netcdf_unified_reader.h     # 继承 UnifiedDataReader
    │       ├── netcdf_unified_reader.cpp
    │       └── internal/                   # NetCDF内部辅助组件 (保持封装, 内容重构)
    │           ├── netcdf_file_processor.h/cpp
    │           ├── netcdf_metadata_manager.h/cpp
    │           ├── io/ (netcdf_attribute_io, netcdf_variable_io etc.)
    │           └── parsing/ (netcdf_cf_conventions, netcdf_coordinate_system_parser (精简后) etc.)
    └── streaming/                          # (新增) 流式处理实现
        ├── streaming_data_processor.h
        └── streaming_data_processor.cpp
```

### 2.2 删除文件清单 (~2400+ 行)
```
core_services_impl/data_access_service/
├── include/core_services/data_access/
│   └── ❌ boost_future_config.h                        # (若不再需要)
│   └── ❌ readers/data_reader_common.h                # (内容迁移或删除)
│   └── ❌ i_data_reader_impl.h                         # (被新API取代)
└── src/impl/
    ├── cache/
    │   ├── ❌ data_chunk_cache.h                       (约597行)
    │   ├── ❌ data_chunk_cache.cpp
    │   ├── ❌ reader_cache.h                          (约424行)
    │   ├── ❌ reader_cache.cpp
    │   ├── ❌ metadata_cache.h                        (约253行)
    │   ├── ❌ metadata_cache.cpp
    │   ├── ❌ netcdf_cache_manager.h                  (约429行)
    │   ├── ❌ netcdf_cache_manager.cpp
    │   └── ❌ cache_manager_template.h
    ├── factory/
    │   ├── ❌ reader_factory.h                         # (功能被 ReaderRegistry 取代)
    │   └── ❌ reader_factory.cpp
    └── readers/netcdf/parsing/
        ├── ❌ netcdf_time_processor.h                  (约200行)
        └── ❌ netcdf_time_processor.cpp
```
*注：原 `raw_data_access_service_impl.h/cpp` 文件将被重命名并彻底重构为 `data_access_service_impl.h/cpp`。部分 `gdal` 和 `netcdf` 内部辅助文件也会被大幅修改或精简。*

### 2.3 新增文件清单 (~1800 - 2000 行)
*详见2.1目标目录结构中标注的 (新增) 和 (新增/重构) 部分。核心新增包括：*
```
✨ include/core_services/data_access/common_types.h
✨ include/core_services/data_access/error_codes.h
✨ include/core_services/data_access/api/* (所有新接口)
✨ src/cache/* (统一缓存相关)
✨ src/async/* (统一异步执行器相关)
✨ src/time/* (CF时间提取器相关)
✨ src/readers/core/* (读取器核心组件相关)
✨ src/streaming/* (流式处理器相关)
```

### 2.4 主要重构文件清单 (大量代码修改)
```
🔄 src/data_access_service_impl.h/cpp (原 raw_data_access_service_impl.h/cpp)
🔄 src/readers/gdal/gdal_raster_reader.h/cpp       # 重构为 GdalUnifiedReader
🔄 src/readers/gdal/gdal_vector_reader.h/cpp       # 重构为 GdalUnifiedReader (或合并)
🔄 src/readers/gdal/internal/*                     # 内部逻辑调整以适配新接口
🔄 src/readers/netcdf/netcdf_cf_reader.h/cpp       # 重构为 NetCdfUnifiedReader
🔄 src/readers/netcdf/internal/*                   # 内部逻辑调整，特别是时间处理移除
🔄 src/readers/netcdf/parsing/netcdf_coordinate_system_parser.h/cpp # 大幅精简，移除时间处理
```

## 📚 3. 渐进式完全重构执行计划

本计划将整个重构过程分解为6个主要阶段，每个阶段都有明确的目标、任务、交付物和验证标准。

---

### Phase 1: 奠定新架构基础 (预计 Week 1)

**🎯 目标:**
-   建立新一代数据访问接口的清晰定义。
-   搭建统一缓存服务的基础设施框架。
-   建立准确的性能基准，用于后续阶段的优化验证。
-   此阶段不修改任何现有业务逻辑的执行路径，风险最低。

**核心任务:**
1.  **定义核心API接口**: 创建所有新的 `IDataSource`, `IMetadataProvider`, `IDataProvider`, `IStreamingDataProvider` 接口以及相关的请求/响应结构体。
2.  **实现统一缓存框架骨架**: 创建 `ICacheableValue`, `UnifiedDataAccessCache` 的头文件和基础实现框架。
3.  **搭建性能基准测试环境**: 编写测试用例，测量当前系统的关键性能指标（如特定操作的响应时间、缓存命中率、内存使用）。
4.  **创建通用类型与错误码文件**: 整理并创建 `common_types.h` 和 `error_codes.h`。

**详细步骤与代码实现指引:**

1.  **创建目录结构 (include)**:
    ```bash
    mkdir -p core_services_impl/data_access_service/include/core_services/data_access/api
    ```
2.  **创建接口文件**:
    *   `core_services_impl/data_access_service/include/core_services/data_access/api/i_data_source.h` (内容如前一方案定义)
    *   `core_services_impl/data_access_service/include/core_services/data_access/api/i_metadata_provider.h` (内容如前一方案定义)
    *   `core_services_impl/data_access_service/include/core_services/data_access/api/i_data_provider.h` (定义 `readGridDataAsync` 和 `readFeatureCollectionAsync` 等，使用请求结构体)
```cpp
        // i_data_provider.h (示例)
#pragma once
        #include <memory>
#include <boost/future.hpp>
        #include "data_access_requests.h"
        #include "data_access_responses.h" // 假设GridData, FeatureCollection定义在此

namespace oscean::core_services::data_access::api {
        class IDataProvider {
public:
            virtual ~IDataProvider() = default;
            virtual boost::future<std::shared_ptr<GridData>> readGridDataAsync(
                const GridReadRequest& request) = 0;
            virtual boost::future<FeatureCollection> readFeatureCollectionAsync(
                const FeatureReadRequest& request) = 0;
            // 可选: virtual boost::future<RawVariableData> readRawVariableDataAsync(
            //    const RawVariableReadRequest& request) = 0;
        };
        }
        ```
    *   `core_services_impl/data_access_service/include/core_services/data_access/api/i_streaming_data_provider.h` (定义 `streamVariableDataAsync`, `getNextDataChunkAsync` 等)
```cpp
        // i_streaming_data_provider.h (示例)
#pragma once
        #include <functional>
        #include <boost/future.hpp>
        #include "streaming_types.h" 
        #include "data_access_responses.h" // For DataChunk

namespace oscean::core_services::data_access::api {
        class IStreamingDataProvider {
        public:
            virtual ~IStreamingDataProvider() = default;
            virtual boost::future<void> streamVariableDataAsync(
                const std::string& variableName,
                std::function<boost::future<bool>(DataChunk)> chunkProcessor,
                const StreamingOptions& options = {}) = 0;
            // 可选 pull 模式
            // virtual boost::future<std::optional<DataChunk>> getNextDataChunkAsync(
            //    const std::string& streamHandle, 
            //    const StreamingOptions& options = {}) = 0; 
        };
        }
        ```
    *   `core_services_impl/data_access_service/include/core_services/data_access/api/data_access_requests.h` (包含 `GridReadRequest`, `FeatureReadRequest`, `RawVariableReadRequest`)
    *   `core_services_impl/data_access_service/include/core_services/data_access/api/data_access_responses.h` (包含 `GridData`, `FeatureCollection`, `DataChunk` 等核心数据结构)
    *   `core_services_impl/data_access_service/include/core_services/data_access/api/streaming_types.h` (包含 `StreamingOptions`, `BackpressureControl` (若需要), `AdaptiveChunkingConfig` (若需要))
3.  **创建通用类型与错误码文件**:
    *   `core_services_impl/data_access_service/include/core_services/data_access/common_types.h`
    *   `core_services_impl/data_access_service/include/core_services/data_access/error_codes.h`
4.  **创建目录结构 (src)**:
    ```bash
    mkdir -p core_services_impl/data_access_service/src/cache
    mkdir -p core_services_impl/data_access_service/src/async
    mkdir -p core_services_impl/data_access_service/src/time
    mkdir -p core_services_impl/data_access_service/src/readers/core
    mkdir -p core_services_impl/data_access_service/src/streaming
    ```
5.  **创建缓存基础设施骨架**:
    *   `core_services_impl/data_access_service/src/cache/i_cacheable_value.h`:
```cpp
#pragma once
        #include <cstddef>
namespace oscean::core_services::data_access::cache {
class ICacheableValue {
public:
    virtual ~ICacheableValue() = default;
    virtual size_t getSizeInBytes() const = 0;
            virtual void onEviction() {} // Callback on eviction
        };
        }
        ```
    *   `core_services_impl/data_access_service/src/cache/unified_data_access_cache.h` (头文件框架，如前一方案定义，依赖 `common_utils/cache/i_cache_manager.h`)
    *   `core_services_impl/data_access_service/src/cache/unified_data_access_cache.cpp` (创建空的实现或基础构造函数)
6.  **建立性能基准测试**:
    *   在测试目录下 (e.g., `tests/data_access/`) 创建 `performance_baseline_tests.cpp`。
    *   使用现有 `RawDataAccessServiceImpl` (或其接口 `IDataAccessService`)。
    *   针对几种典型场景 (小文件读取、大数据块读取、元数据查询、高频访问等) 编写测试。
    *   测量并记录：平均响应时间、P95/P99响应时间、特定操作的内存峰值、当前缓存（如果可观测）的命中率。
    *   确保这些测试是可重复的。

**涉及文件 (Phase 1):**
*   **新增**:
    *   `include/core_services/data_access/api/*` (所有新接口头文件)
    *   `include/core_services/data_access/common_types.h`
    *   `include/core_services/data_access/error_codes.h`
    *   `src/cache/i_cacheable_value.h`
    *   `src/cache/unified_data_access_cache.h`
    *   `src/cache/unified_data_access_cache.cpp` (骨架)
    *   `tests/data_access/performance_baseline_tests.cpp`
*   **修改**: 无生产代码修改。
*   **删除**: 无。

**风险控制与验证 (Phase 1):**
*   **代码审查**: 对所有新接口定义进行严格审查，确保其合理性、完整性和前瞻性。
*   **编译通过**: 确保所有新增的头文件和骨架实现能够编译通过。
*   **性能基准**: 成功建立可重复的性能基准测试，并记录初始性能数据。
*   **无功能影响**: 由于此阶段不涉及修改现有服务逻辑，现有系统功能应完全不受影响。所有现有单元测试和集成测试必须100%通过。

**预期成果 (Phase 1):**
1.  一套完整、经过审查的新数据访问API定义。
2.  统一缓存服务 `UnifiedDataAccessCache` 的基础框架搭建完成。
3.  一套可量化、可重复的性能基准测试数据。
4.  项目代码结构已初步调整，为后续阶段的实现做好准备。

---

### Phase 2: 缓存系统统一与集成 (预计 Week 2)

**🎯 目标:**
-   完全实现 `UnifiedDataAccessCache`，利用 `common_utils::cache` 提供的能力，支持多种缓存策略（LRU, LFU, TTL 等）和不同的缓存区域（如数据块缓存、读取器实例缓存、元数据缓存）。
-   将 `DataAccessServiceImpl` (原 `RawDataAccessServiceImpl`) 中的所有缓存逻辑迁移到使用 `UnifiedDataAccessCache`。
-   彻底删除项目中所有旧的、独立的缓存实现代码，减少代码冗余。
-   验证新缓存系统的功能正确性、性能提升（对比Phase 1基准）和内存使用效率。

**核心任务:**
1.  **详细实现 `UnifiedDataAccessCache`**: 包括初始化逻辑、缓存区域的创建与管理、异步的get/put操作、统计信息获取等。
2.  **重构 `DataAccessServiceImpl`**: 修改其构造函数以接收 `UnifiedDataAccessCache` 实例，并更新所有数据和元数据读取方法，使其通过新缓存系统进行存取。
3.  **删除旧缓存代码**: 安全移除 `src/impl/cache/` 目录下的所有旧缓存文件。
4.  **编写单元测试和集成测试**: 针对 `UnifiedDataAccessCache` 的不同策略和 `DataAccessServiceImpl` 中缓存相关的逻辑进行测试。
5.  **性能对比测试**: 运行Phase 1建立的性能基准测试，验证新缓存系统带来的性能提升。

**详细步骤与代码实现指引:**

1.  **实现 `UnifiedDataAccessCache` (`unified_data_access_cache.cpp`)**: 
    *   **构造函数**: 接收 `common_utils::logging::Logger`。
    *   **`initialize` 方法**: 
        *   接收全局缓存配置 `common_utils::cache::CacheConfig` 和可选的区域特定配置 `std::map<std::string, common_utils::cache::CacheConfig>`。
        *   根据配置创建并存储不同类型的缓存实例 (`ICacheManager<KeyType, ValueType>`) 到 `cacheRegions_` map中。例如：
            *   `data_chunks`: 可能使用 `LRUCacheStrategy` 或 `AdaptiveCacheStrategy`，配置较大的 `maxSizeBytes`。
            *   `readers`: 可能使用 `LFUCacheStrategy`，配置 `maxItems`，用于缓存打开的 `IDataSource` 实例。
            *   `metadata`: 可能使用 `TTLCacheStrategy`，配置默认的 `defaultTtl`，用于缓存文件元数据。
        *   使用 `common_utils::cache::CacheFactory` (如果存在) 或直接实例化策略。
```cpp
        // unified_data_access_cache.cpp (initialize 示例片段)
#include "unified_data_access_cache.h"
        #include "common_utils/cache/lru_cache_strategy.h" // 或其他策略
        #include "common_utils/cache/lfu_cache_strategy.h"
        #include "common_utils/cache/ttl_cache_strategy.h"
        #include "common_utils/cache/cache_factory.h" // 假设有工厂
        #include "../api/data_access_responses.h" // For GridData etc.
        #include "../api/i_data_source.h" // For IDataSource

namespace oscean::core_services::data_access::cache {

UnifiedDataAccessCache::UnifiedDataAccessCache(
    std::shared_ptr<common_utils::logging::Logger> logger)
            : logger_(std::move(logger)) {}

void UnifiedDataAccessCache::initialize(
    const common_utils::cache::CacheConfig& globalConfig,
    const std::map<std::string, common_utils::cache::CacheConfig>& regionConfigs) {
    std::unique_lock<std::shared_mutex> lock(regionsMutex_);
    
            // 示例: 数据块缓存区域 (DataChunkKey 在 common_types.h 中定义)
            auto dataChunkConf = regionConfigs.count(DATA_CHUNKS_REGION) ? 
                                 regionConfigs.at(DATA_CHUNKS_REGION) : globalConfig;
            // 强制或推荐策略，例如LRU
            dataChunkConf.strategyType = dataChunkConf.strategyType == common_utils::cache::CacheStrategyType::NONE ? 
                                         common_utils::cache::CacheStrategyType::LRU : dataChunkConf.strategyType;
            dataChunkConf.name = DATA_CHUNKS_REGION;
            auto dataChunkCache = common_utils::cache::CacheFactory::createCache<DataChunkKey, std::shared_ptr<GridData>>(
                dataChunkConf, logger_); // GridData 定义在 data_access_responses.h
            cacheRegions_[DATA_CHUNKS_REGION] = dataChunkCache;
            logger_->info("Cache region '{}' initialized with strategy {} and size {}MB", 
                          DATA_CHUNKS_REGION, dataChunkConf.strategyType, dataChunkConf.maxSizeBytes / (1024*1024));
        
            // 示例: 读取器实例缓存区域
            auto readerConf = regionConfigs.count(READER_INSTANCES_REGION) ? 
                              regionConfigs.at(READER_INSTANCES_REGION) : globalConfig;
            readerConf.strategyType = common_utils::cache::CacheStrategyType::LFU;
            readerConf.maxItems = readerConf.maxItems.value_or(50); // Default max 50 readers
            readerConf.name = READER_INSTANCES_REGION;
            auto readerCache = common_utils::cache::CacheFactory::createCache<std::string, std::shared_ptr<api::IDataSource>>(
                readerConf, logger_); // Key: filePath, Value: reader instance
            cacheRegions_[READER_INSTANCES_REGION] = readerCache;
            logger_->info("Cache region '{}' initialized with strategy {} and max items {}", 
                          READER_INSTANCES_REGION, readerConf.strategyType, readerConf.maxItems.value());

            // 示例: 元数据缓存区域 (FileMetadataKey 在 common_types.h 中定义)
            auto metadataConf = regionConfigs.count(FILE_METADATA_REGION) ? 
                               regionConfigs.at(FILE_METADATA_REGION) : globalConfig;
            metadataConf.strategyType = common_utils::cache::CacheStrategyType::TTL;
            metadataConf.defaultTtl = metadataConf.defaultTtl.value_or(std::chrono::minutes(30));
            metadataConf.name = FILE_METADATA_REGION;
            auto metadataCache = common_utils::cache::CacheFactory::createCache<FileMetadataKey, FileMetadata>(
                metadataConf, logger_); // FileMetadata 定义在 common_types.h 或 responses.h
            cacheRegions_[FILE_METADATA_REGION] = metadataCache;
            logger_->info("Cache region '{}' initialized with strategy {} and TTL {}s", 
                          FILE_METADATA_REGION, metadataConf.strategyType, metadataConf.defaultTtl.value().count());
            
            logger_->info("UnifiedDataAccessCache initialized with {} regions.", cacheRegions_.size());
        }
        
        // ... 实现 getAsync, putAsync, clearRegionAsync, getRegionStats 等模板方法 ...
        // 这些方法内部会根据 regionName 获取对应的 ICacheManager 实例并调用其方法。
        // 需要注意类型转换和错误处理。

        // 在 .h 文件中定义常量
        // static constexpr const char* DATA_CHUNKS_REGION = "data_chunks";
        // static constexpr const char* READER_INSTANCES_REGION = "reader_instances";
        // static constexpr const char* FILE_METADATA_REGION = "file_metadata";

} // namespace
```
    *   **`getAsync`, `putAsync` 模板方法**: 实现异步的缓存获取和存入，内部通过regionName找到对应的 `ICacheManager` 实例，并调用其异步方法。确保线程安全和正确的类型转换。
    *   **`clearRegionAsync`, `getRegionStats`**: 实现对特定缓存区域的清理和统计信息获取。
2.  **定义缓存键类型 (在 `include/core_services/data_access/common_types.h` 中)**:
    *   `DataChunkKey`: 用于数据块缓存，应包含文件路径、变量名、切片范围、目标CRS等能唯一标识一个数据请求的元素。需要实现 `std::hash` 和 `operator==`。
    *   `FileMetadataKey`: 用于文件元数据缓存，通常是文件路径，但也可能包含其他标识符（如文件修改时间，以处理文件更新）。
3.  **重构 `DataAccessServiceImpl` (原 `RawDataAccessServiceImpl`)**:
    *   在 `data_access_service_impl.h` 中添加 `std::shared_ptr<cache::UnifiedDataAccessCache> unifiedCache_;` 成员。
    *   修改构造函数，注入 `UnifiedDataAccessCache` 的实例。
    *   **替换读取数据逻辑**: 
        *   在如 `readGridDataAsync` 方法中，先根据请求参数构造 `DataChunkKey`。
        *   调用 `unifiedCache_->getAsync<DataChunkKey, std::shared_ptr<GridData>>(DATA_CHUNKS_REGION, cacheKey)`。
        *   如果缓存命中，直接返回结果。
        *   如果未命中，则执行实际的数据读取逻辑（将在后续Phase重构读取器时完善），读取后将结果通过 `unifiedCache_->putAsync(...)` 存入缓存。
```cpp
        // data_access_service_impl.cpp (readGridDataAsync 示例片段)
        boost::future<std::shared_ptr<GridData>> DataAccessServiceImpl::readGridDataAsync(
            const api::GridReadRequest& request /* 使用新接口的请求对象 */) {
        
            // 假设 request 包含 filePath, variableName, sliceRanges, targetCrs 等
            // 构造 DataChunkKey (定义在 common_types.h)
            cache::DataChunkKey cacheKey { 
                resolveFilePath(request.filePath), // 确保路径规范化 
                request.variableName,
                request.sliceRanges.value_or(std::vector<IndexRange>()), // 提供默认值
                request.targetCRS 
            };
        
            return unifiedCache_->getAsync<cache::DataChunkKey, std::shared_ptr<GridData>>(
                cache::UnifiedDataAccessCache::DATA_CHUNKS_REGION, cacheKey)
                .then(boost::launch::deferred, // 或者使用线程池
                    [this, request, cacheKey](boost::future<std::optional<std::shared_ptr<GridData>>> futureValue) {
                    auto cachedDataOpt = futureValue.get();
                    if (cachedDataOpt && *cachedDataOpt) {
                        logger_->debug("Data chunk cache hit for key: {}", cacheKey.toString());
                        // TODO: 更新缓存统计（可选，ICacheManager内部可能已处理）
                        return boost::make_ready_future(*cachedDataOpt);
                    }
        
                    logger_->debug("Data chunk cache miss for key: {}. Loading from source.", cacheKey.toString());
                    // 实际的加载逻辑 (当前可能还是旧的读取方式，后续Phase会重构这里)
                    return this->loadAndCacheGridData(request, cacheKey); 
                }).unwrap();
        }
        
        // 新增辅助方法
        boost::future<std::shared_ptr<GridData>> DataAccessServiceImpl::loadAndCacheGridData(
            const api::GridReadRequest& request, 
            const cache::DataChunkKey& cacheKey) {
            // ... 旧的读取逻辑，或者调用将在Phase 5中重构的读取器 ...
            // Puntero: std::shared_ptr<IDataReaderImpl> reader = readerFactory_->getReader(filePath);
            // auto data = reader->readGridData(...);
            // 
            // 假设已获取 data (std::shared_ptr<GridData>)
            // return boost::make_ready_future(data) 
            //    .then([this, cacheKey, data](boost::future<std::shared_ptr<GridData>> /*futData*/) {
            //        unifiedCache_->putAsync(cache::UnifiedDataAccessCache::DATA_CHUNKS_REGION, cacheKey, data, std::nullopt /*ttl*/);
            //        return data;
            //    });
            // 此部分实现依赖后续读取器重构，暂时保留现有读取逻辑并包装缓存
            // 示例: 假设 loadFromSourceInternal 是旧的读取逻辑
            return loadFromSourceInternal(request)
                .then(boost::launch::deferred, [this, cacheKey](boost::future<std::shared_ptr<GridData>> dataFuture) {
                        auto data = dataFuture.get();
                    if (data) { // 确保数据有效再缓存
                         // 计算data的大小，如果GridData实现了ICacheableValue则更好
                        size_t dataSize = 0; 
                        auto cacheableData = std::dynamic_pointer_cast<cache::ICacheableValue>(data);
                        if(cacheableData) dataSize = cacheableData->getSizeInBytes();
                        else { /* 估算大小 */ }

                        // 使用common_utils的CacheItem
                        // auto cacheItem = common_utils::cache::CacheItem<std::shared_ptr<GridData>>(data, dataSize);
                        // unifiedCache_->putAsync(cache::UnifiedDataAccessCache::DATA_CHUNKS_REGION, cacheKey, std::move(cacheItem));
                        // 或者直接传递值，如果ICacheManager支持
                        unifiedCache_->putAsync(cache::UnifiedDataAccessCache::DATA_CHUNKS_REGION, cacheKey, data, std::nullopt);

                        logger_->debug("Data chunk stored in cache for key: {}", cacheKey.toString());
                    }
                        return data;
                    });
        }
        ```
    *   **替换获取元数据逻辑**: 类似地，使用 `FileMetadataKey` 和 `FILE_METADATA_REGION` 区域来缓存文件级别的元数据。
    *   **替换读取器实例管理**: 如果之前有读取器实例的缓存（如 `ReaderCache`），也将其替换为使用 `UnifiedDataAccessCache` 的 `READER_INSTANCES_REGION`。
4.  **删除旧的缓存文件**:
    ```bash
    rm -rf core_services_impl/data_access_service/src/impl/cache/*
    # 验证 src/impl/ 目录是否还存在，如果cache是其下最后一个子目录，则可能需要调整路径
    # 根据2.1的最终目录结构，新的缓存文件在 core_services_impl/data_access_service/src/cache/
    ```
    确保在版本控制中正确删除这些文件。
5.  **编写单元测试**: 
    *   为 `UnifiedDataAccessCache` 编写单元测试 (`tests/data_access/cache/unified_data_access_cache_tests.cpp`)。
        *   测试不同缓存区域的创建和配置。
        *   测试 LRU, LFU, TTL 策略的正确性（例如，验证项的驱逐顺序、过期）。
        *   测试并发get/put操作的线程安全性。
        *   测试统计信息的准确性。
    *   为 `DataAccessServiceImpl` 中与缓存相关的部分编写单元测试，使用Mock的 `UnifiedDataAccessCache` 来验证其交互逻辑。
6.  **集成与性能测试**:
    *   重新运行 Phase 1 中建立的 `performance_baseline_tests.cpp`。
    *   比较新旧缓存系统的性能指标：响应时间、缓存命中率（通过 `getRegionStats` 获取）。
    *   监控内存使用情况，确保没有意外的内存增长。

**涉及文件 (Phase 2):**
*   **新增/实现**: 
    *   `src/cache/unified_data_access_cache.cpp` (完整实现)
    *   `tests/data_access/cache/unified_data_access_cache_tests.cpp`
*   **修改**: 
    *   `include/core_services/data_access/common_types.h` (添加缓存键定义 `DataChunkKey`, `FileMetadataKey`)
    *   `src/data_access_service_impl.h` (添加 `UnifiedDataAccessCache` 成员，修改构造函数)
    *   `src/data_access_service_impl.cpp` (全面使用 `UnifiedDataAccessCache` 替换旧缓存逻辑)
    *   `tests/data_access/performance_baseline_tests.cpp` (用于对比测试，可能需要适配新的统计获取方式)
    *   CMakeLists.txt (添加新文件，移除旧文件)
*   **删除**: 
    *   所有位于 `core_services_impl/data_access_service/src/impl/cache/` 下的旧缓存文件 (如 `data_chunk_cache.h/cpp`, `reader_cache.h/cpp`, `metadata_cache.h/cpp`, `netcdf_cache_manager.h/cpp`, `cache_manager_template.h`)。

**风险控制与验证 (Phase 2):**
*   **逐步替换**: 可以考虑先替换一个缓存类型（如数据块缓存），验证通过后再替换其他类型。
*   **功能开关 (可选)**: 如果系统复杂，可以考虑引入一个临时的配置开关，允许在旧缓存和新缓存间切换，以便快速回滚。
*   **严格测试**: 单元测试覆盖率应尽可能高。集成测试应覆盖所有主要的用例。
*   **性能监控**: 密切关注性能测试结果，任何性能下降都需要调查。目标是性能提升或持平（在某些复杂场景下）。
*   **内存分析**: 使用内存分析工具检查是否存在内存泄漏或不合理的内存占用。
*   **代码审查**: 对 `UnifiedDataAccessCache` 的实现和 `DataAccessServiceImpl` 的修改进行彻底审查。

**预期成果 (Phase 2):**
1.  一个功能完整、基于 `common_utils` 的 `UnifiedDataAccessCache` 服务。
2.  `DataAccessServiceImpl` 完全迁移到使用新的统一缓存系统。
3.  约 1000-1500 行旧的冗余缓存代码被成功移除。
4.  通过单元测试和集成测试验证了新缓存系统的正确性和稳定性。
5.  性能测试结果显示缓存命中率和响应时间得到改善（或至少不差于基准），内存使用更加高效可控。
6.  为后续的异步框架统一和读取器重构奠定了坚实的缓存基础。

---
*这是Phase 2的详细计划。请您审阅。确认后，我将继续提供Phase 3（异步框架统一）的计划。* 

---

### Phase 3: 异步框架统一 (预计 Week 3)

**🎯 目标:**
-   设计并实现 `UnifiedAsyncExecutor`，作为模块内所有异步操作的统一入口，封装 `common_utils::async::TaskManager` 的使用。
-   在 `DataAccessServiceImpl` 中全面采用 `UnifiedAsyncExecutor` 来执行所有耗时的I/O操作和计算任务，取代直接的 `boost::async` 或自定义线程池调用。
-   消除 `boost_future_config.h` 等与旧异步实现相关的配置和代码。
-   提升异步任务的可管理性、可配置性（如优先级、重试策略）和整体性能。

**核心任务:**
1.  **实现 `UnifiedAsyncExecutor`**: 定义其接口，包括执行通用异步任务、以及可能针对读取器操作的专用方法。实现中应合理利用 `TaskManager` 的线程池和任务调度能力。
2.  **重构 `DataAccessServiceImpl`**: 修改其构造函数以接收 `UnifiedAsyncExecutor` 实例。更新所有异步方法（如 `readGridDataAsync`, `getMetadataAsync` 等），使其通过新的执行器提交任务。
3.  **删除旧的异步代码和配置**: 移除如 `boost_future_config.h` 及服务内部直接创建 `boost::future` 的逻辑。
4.  **编写单元测试**: 针对 `UnifiedAsyncExecutor` 及其与 `TaskManager` 的交互进行测试。
5.  **验证异步性能和行为**: 确保异步操作的正确性，以及在负载下的性能表现不劣于之前，并分析资源使用情况。

**详细步骤与代码实现指引:**

1.  **创建 `UnifiedAsyncExecutor`**: (`src/async/`)
    *   `unified_async_executor.h`:
```cpp
#pragma once
#include <memory>
#include <functional>
#include <string>
        #include <optional>
#include <boost/future.hpp>
#include "common_utils/async/task_manager.h"
#include "common_utils/async/async_priority.h"
#include "common_utils/async/retry_policy.h"
#include "common_utils/logging/logger.h"

namespace oscean::core_services::data_access::async {

class UnifiedAsyncExecutor {
public:
    UnifiedAsyncExecutor(
        std::shared_ptr<common_utils::async::TaskManager> taskManager,
        std::shared_ptr<common_utils::logging::Logger> logger);
    
    template<typename ReturnType, typename TaskFunc>
            boost::future<ReturnType> submit(
        TaskFunc&& task,
                const std::string& taskDescription = "DataAccessGenericTask",
                common_utils::async::AsyncPriority priority = common_utils::async::AsyncPriority::NORMAL,
        std::optional<common_utils::async::RetryPolicy> retryPolicy = std::nullopt,
                // 可选：是否允许任务窃取等高级TaskManager特性
                bool allowStealing = true 
            );
        
            // 可以考虑为特定操作类型（如IO密集型、CPU密集型）提供不同配置的提交方法
            // template<typename ReturnType, typename TaskFunc>
            // boost::future<ReturnType> submitIOBoundTask(...);
    
private:
    std::shared_ptr<common_utils::async::TaskManager> taskManager_;
    std::shared_ptr<common_utils::logging::Logger> logger_;
    
            // 辅助函数，将 AsyncPriority 转换为 TaskManager 可能需要的参数
            // common_utils::async::TaskAttributes priorityToAttributes(common_utils::async::AsyncPriority priority);
};

} // namespace
```
    *   `unified_async_executor.cpp`:
        ```cpp
        #include "unified_async_executor.h"
        
        namespace oscean::core_services::data_access::async {
        
        UnifiedAsyncExecutor::UnifiedAsyncExecutor(
            std::shared_ptr<common_utils::async::TaskManager> taskManager,
            std::shared_ptr<common_utils::logging::Logger> logger)
            : taskManager_(std::move(taskManager)), logger_(std::move(logger)) {
            if (!taskManager_) {
                throw std::invalid_argument("TaskManager cannot be null for UnifiedAsyncExecutor");
            }
            logger_->info("UnifiedAsyncExecutor initialized.");
        }
        
        template<typename ReturnType, typename TaskFunc>
        boost::future<ReturnType> UnifiedAsyncExecutor::submit(
            TaskFunc&& task, 
            const std::string& taskDescription,
            common_utils::async::AsyncPriority priority,
            std::optional<common_utils::async::RetryPolicy> retryPolicy, // retryPolicy暂未直接集成进TaskManager submit
            bool allowStealing) {
            
            // TaskManager::submit 通常返回 std::future, 需要适配为 boost::future
            // 或者 TaskManager 直接支持返回 boost::future (需确认 common_utils 实现)
            // 假设 TaskManager::submit 返回 std::future
            
            // 构造 TaskAttributes (如果 TaskManager 需要)
            common_utils::async::TaskAttributes attributes;
            attributes.description = taskDescription;
            attributes.priority = priority; 
            // attributes.allowStealing = allowStealing; 
            // attributes.retryPolicy = retryPolicy; // TaskManager 可能内部处理重试或返回可重试的future

            // 封装一层以适配返回类型和可能的重试逻辑 (如果TaskManager不直接支持)
            // 如果 TaskManager 的 submit 直接支持 boost::future 和 RetryPolicy，则可以直接调用
            
            // 简化版：直接提交，不处理重试策略于此层面，依赖TaskManager或调用者
            // 注意：common_utils::TaskManager::submit 的确切签名和能力是关键
            auto stdFuture = taskManager_->submit<ReturnType>(std::forward<TaskFunc>(task), attributes);

            // 将 std::future 转换为 boost::future
            // 这通常需要一个 promise 和一个在 stdFuture 完成时设置 promise 的机制
            // 或者，如果 common_utils 提供了转换函数，则使用它。
            // 简单的转换（无异常传播细节处理的简化示例）：
            boost::promise<ReturnType> promise;
            boost::future<ReturnType> boostFuture = promise.get_future();
            
            // 在TaskManager的线程中执行以下lambda来桥接
            taskManager_->submit<void>([stdFuture = std::move(stdFuture), promise = std::move(promise)]() mutable {
                try {
                    if constexpr (std::is_same_v<ReturnType, void>) {
                        stdFuture.get();
                        promise.set_value();
                    } else {
                        promise.set_value(stdFuture.get());
                    }
                } catch (...) {
                    promise.set_exception(boost::current_exception());
                }
            }, "FutureBridgeTask"); // 提交一个辅助任务来完成桥接

            return boostFuture;
            
            // ---- 更理想的情况：TaskManager 直接支持 boost::future ----
            // return taskManager_->submitBoost<ReturnType>(std::forward<TaskFunc>(task), attributes);
        }

        // 需要为模板方法在cpp中提供显式实例化或将其完全移至头文件
        // 显式实例化示例 (根据实际使用的类型):
        template boost::future<std::shared_ptr<oscean::core_services::data_access::api::GridData>> 
        UnifiedAsyncExecutor::submit<std::shared_ptr<oscean::core_services::data_access::api::GridData>, std::function<std::shared_ptr<oscean::core_services::data_access::api::GridData>()>>(
            std::function<std::shared_ptr<oscean::core_services::data_access::api::GridData>()>&&, 
            const std::string&, 
            common_utils::async::AsyncPriority, 
            std::optional<common_utils::async::RetryPolicy>, 
            bool);
        // ... 为其他需要的类型实例化 ...

        } // namespace
        ```
        *注: `std::future` 到 `boost::future` 的转换，以及重试策略的集成，依赖 `common_utils::TaskManager` 的具体实现。上述代码提供了一种可能的桥接方式，但理想情况下 `TaskManager` 应直接支持或提供工具类。如果模板方法主体较复杂或依赖较多，建议将其保留在头文件中。*

2.  **重构 `DataAccessServiceImpl`**: 
    *   在 `data_access_service_impl.h` 中添加 `std::shared_ptr<async::UnifiedAsyncExecutor> asyncExecutor_;` 成员。
    *   修改构造函数，注入 `UnifiedAsyncExecutor` 的实例。
    *   **更新异步方法**: 例如 `loadAndCacheGridData` (在Phase 2中引入的辅助方法，用于实际加载数据) 或直接修改如 `readGridDataAsync` 的核心加载部分。
```cpp
        // data_access_service_impl.cpp (loadAndCacheGridData 示例片段)
        boost::future<std::shared_ptr<GridData>> DataAccessServiceImpl::loadAndCacheGridData(
            const api::GridReadRequest& request, 
            const cache::DataChunkKey& cacheKey) {
            
            // 将实际的数据加载操作封装为一个 lambda
            auto loadOperation = [this, request]() -> std::shared_ptr<GridData> {
                // logger_->debug("Executing actual data load for: {}", request.filePath);
                // ... 这里是之前直接执行的I/O密集型或CPU密集型的数据读取逻辑 ...
                // ... (例如，调用底层的GDAL/NetCDF读取函数) ...
                // ... 这个逻辑将在Phase 5被进一步重构为通过UnifiedDataReader执行 ...
                // 假设: auto data = performActualReadOperation(request);
                // return data;
                
                // 临时的伪实现，直到Phase 5
                if (request.filePath.empty()) { // 示例错误处理
                     throw std::runtime_error("File path is empty in GridReadRequest");
                }
                logger_->info("(Pseudo-load) Loading data for {} variable {}", request.filePath, request.variableName);
                // 返回一个伪造的GridData用于流程测试
                auto dummyData = std::make_shared<GridData>(); 
                // dummyData->tensor = ... 创建一些数据 ...
                return dummyData; 
            };
        
            // 使用 UnifiedAsyncExecutor 提交任务
            return asyncExecutor_->submit<std::shared_ptr<GridData>>(
                std::move(loadOperation), 
                "LoadGridData:" + request.filePath + "/" + request.variableName,
                common_utils::async::AsyncPriority::HIGH // 根据情况调整优先级
            ).then(boost::launch::deferred, [this, cacheKey](boost::future<std::shared_ptr<GridData>> dataFuture) {
                // ... (缓存逻辑同Phase 2) ...
                 auto data = dataFuture.get(); // 获取异步加载的结果
                 if (data) {
                     unifiedCache_->putAsync(cache::UnifiedDataAccessCache::DATA_CHUNKS_REGION, cacheKey, data, std::nullopt);
                     logger_->debug("Data chunk stored in cache for key: {}", cacheKey.toString());
                 }
                 return data;
            });
        }
        ```
    *   确保所有的 `then` 和 `unwrap` 调用在合适的执行上下文中（例如，如果回调很快，可以用 `boost::launch::deferred`；如果回调也耗时，也应通过 `asyncExecutor_` 调度）。

3.  **删除旧的异步配置和代码**: 
    *   从 `CMakeLists.txt` 和项目中删除 `include/core_services/data_access/boost_future_config.h` (如果它只用于此模块的异步配置)。
    *   检查 `DataAccessServiceImpl` 和其他相关类，移除所有直接的 `boost::async` 调用或自定义的线程管理代码，替换为通过 `UnifiedAsyncExecutor`。

4.  **编写单元测试**: (`tests/data_access/async/unified_async_executor_tests.cpp`)
    *   测试 `UnifiedAsyncExecutor::submit` 是否能成功将任务提交给 `MockTaskManager`。
    *   验证不同优先级的任务是否能被正确传递。
    *   测试当 `TaskManager` 返回成功或失败时，`UnifiedAsyncExecutor` 是否能正确传递结果或异常。
    *   为 `DataAccessServiceImpl` 中使用 `UnifiedAsyncExecutor` 的方法编写单元测试，Mock `UnifiedAsyncExecutor`。

5.  **集成与性能测试**: 
    *   运行所有现有集成测试，确保系统功能和异步流程的正确性。
    *   再次运行Phase 1建立的性能基准测试。重点关注：
        *   异步操作的端到端响应时间。
        *   系统在高并发请求下的吞吐量和稳定性。
        *   `TaskManager` 的线程池利用率和任务队列情况（如果 `TaskManager` 提供相应监控接口）。
    *   目标是性能不显著下降，理想情况下因更优的线程管理而有所提升或更稳定。

**涉及文件 (Phase 3):**
*   **新增/实现**: 
    *   `src/async/unified_async_executor.h`
    *   `src/async/unified_async_executor.cpp`
    *   `tests/data_access/async/unified_async_executor_tests.cpp`
*   **修改**: 
    *   `src/data_access_service_impl.h` (添加 `UnifiedAsyncExecutor` 成员，修改构造函数)
    *   `src/data_access_service_impl.cpp` (所有异步操作通过 `UnifiedAsyncExecutor` 提交)
    *   `tests/data_access/data_access_service_impl_tests.cpp` (适配Mock的 `UnifiedAsyncExecutor`)
    *   CMakeLists.txt (添加新文件，移除旧文件)
*   **删除**: 
    *   `include/core_services/data_access/boost_future_config.h` (如果确认不再被其他地方需要)
    *   服务内部直接创建 `boost::packaged_task` 或调用 `boost::async` 的相关代码。

**风险控制与验证 (Phase 3):**
*   **TaskManager 依赖**: `UnifiedAsyncExecutor` 的实现高度依赖 `common_utils::async::TaskManager` 的接口和行为。确保对其有清晰理解。
*   **Future 类型桥接**: `std::future` 到 `boost::future` 的转换需要小心处理，确保异常和值的正确传播。如果 `TaskManager` 能直接返回 `boost::future` 或提供转换工具，将大大简化实现。
*   **线程模型**: 确保 `UnifiedAsyncExecutor` 的使用与 `TaskManager` 的线程模型（如线程池大小、任务队列行为）相匹配，避免不必要的阻塞或资源竞争。
*   **回调执行上下文**: `boost::future::then()` 的回调默认执行方式可能与预期不同。需要明确指定 `boost::launch`策略，或确保回调本身也通过 `UnifiedAsyncExecutor` 提交，以避免阻塞 `TaskManager` 的工作线程或在非预期线程执行。
*   **性能回归**: 密切监控性能测试，任何显著的性能下降都需要分析是由于执行器本身开销，还是任务提交/调度方式不当。

**预期成果 (Phase 3):**
1.  一个功能完善的 `UnifiedAsyncExecutor`，作为模块内异步任务的统一调度中心。
2.  `DataAccessServiceImpl` 中的异步操作全部迁移至通过 `UnifiedAsyncExecutor` 执行。
3.  移除了约 100-300 行与旧异步实现相关的代码和配置。
4.  异步任务的管理更加集中和规范，为后续引入更复杂的异步控制（如超时、取消、重试）打下基础。
5.  系统在处理并发请求时的稳定性和资源利用率得到改善或保持。

---
*这是Phase 3的详细计划。请您审阅。确认后，我将继续提供Phase 4（时间处理统一）的计划。* 

---

### Phase 4: 时间处理统一 (预计 Week 4)

**🎯 目标:**
-   设计并实现 `CFTimeExtractor`，利用 `common_utils::time` 提供的日历和时间转换能力，专门处理CF约定下的时间坐标（如 "days since YYYY-MM-DD HH:MM:SS" 格式）。
-   在需要进行时间解析和转换的地方（主要是在NetCDF读取器中获取时间范围或处理时间维度时）使用 `CFTimeExtractor`。
-   彻底删除旧的 `NetCDFTimeProcessor` 及其相关的时间处理代码，减少重复实现。
-   确保时间转换的准确性，并符合CF标准约定。

**核心任务:**
1.  **实现 `CFTimeExtractor`**: 定义其接口，使其能够从元数据中提取时间单位、参考日期、日历类型，并能将时间坐标值转换为标准的日期时间对象 (如 `common_utils::time::CalendarTime`)。
2.  **重构NetCDF读取器中的时间处理逻辑**: 当NetCDF读取器（将在Phase 5重构为 `NetCDFUnifiedReader`）需要解析时间信息时，应实例化并使用 `CFTimeExtractor`。
3.  **删除旧的时间处理代码**: 安全移除 `src/impl/readers/netcdf/parsing/netcdf_time_processor.h/cpp`。
4.  **编写单元测试**: 针对 `CFTimeExtractor` 的各种CF时间字符串格式和日历类型进行详尽测试。
5.  **验证时间处理的准确性**: 通过集成测试，确保从NetCDF文件中读取的时间信息与预期一致。

**详细步骤与代码实现指引:**

1.  **创建 `CFTimeExtractor`**: (`src/time/`)
    *   `cf_time_extractor.h`:
```cpp
#pragma once
#include <memory>
#include <string>
        #include <optional>
#include <boost/future.hpp>
        #include "common_utils/time/i_time_extractor.h" // 从 common_utils 继承或类似接口
#include "common_utils/time/calendar_time.h"
#include "common_utils/time/time_range.h"
#include "common_utils/time/calendar_converter.h"
        #include "common_utils/logging/logger.h"
        #include "../api/i_metadata_provider.h" // 用于获取时间变量的属性

namespace oscean::core_services::data_access::time {

/**
         * @brief Extracts time information according to CF conventions.
         * Relies on common_utils::time for actual calendar conversions.
 */
class CFTimeExtractor : public common_utils::time::ITimeExtractor {
public:
    CFTimeExtractor(
                std::weak_ptr<api::IMetadataProvider> metadataProvider, // To fetch 'units' and 'calendar' attributes
                const std::string& timeVariableName, // The name of the time coordinate variable
        std::shared_ptr<common_utils::logging::Logger> logger);
    
            // Parses a CF-compliant time unit string (e.g., "days since 1970-01-01 00:00:00")
            // and a numeric time value, then converts it to CalendarTime.
    boost::future<std::optional<common_utils::time::CalendarTime>> 
                convertCoordinateToCalendarTimeAsync(double timeCoordinateValue) override;
        
            // Converts a vector of time coordinate values.
            boost::future<std::vector<std::optional<common_utils::time::CalendarTime>>>
                convertCoordinatesToCalendarTimesAsync(const std::vector<double>& timeCoordinateValues);
            
            // Extracts the overall time range for the configured time variable.
            boost::future<std::optional<common_utils::time::TimeRange>> getTimeRangeAsync() override;

            // ITimeExtractor (common_utils) MIGHT have these, if so, implement or remove if not needed by data_access
            // boost::future<std::optional<common_utils::time::CalendarTime>> 
            //     extractTimeAsync(const std::string& timeValueString) override; 
            // boost::future<std::optional<common_utils::time::TimeRange>> 
            //     extractTimeRangeAsync(const std::string& timeRangeString) override;
    
private:
            struct TimeAttrs {
                std::string units;
                common_utils::time::CalendarType calendar;
                // Potentially store the parsed reference_datetime from units here
                common_utils::time::CalendarDateTime referenceDateTime;
            };

    std::weak_ptr<api::IMetadataProvider> metadataProvider_;
            std::string timeVariableName_;
    std::shared_ptr<common_utils::logging::Logger> logger_;
            std::shared_ptr<common_utils::time::CalendarConverter> calendarConverter_;
    
            boost::shared_future<std::optional<TimeAttrs>> timeAttrsFuture_; // Cache for attributes
            mutable boost::shared_mutex timeAttrsMutex_; // To protect initialization of timeAttrsFuture_
    
            boost::future<std::optional<TimeAttrs>> fetchTimeAttributesAsync();
            std::optional<TimeAttrs> parseTimeAttributes(const std::vector<MetadataEntry>& attrs);
};

} // namespace
```
    *   `cf_time_extractor.cpp`:
        ```cpp
        #include "cf_time_extractor.h"
        #include "common_utils/time/cf_time_parser.h" // Assuming common_utils provides this parser
        #include "common_utils/string/string_utils.h" // For parsing helps

        namespace oscean::core_services::data_access::time {

        CFTimeExtractor::CFTimeExtractor(
            std::weak_ptr<api::IMetadataProvider> metadataProvider,
            const std::string& timeVariableName,
            std::shared_ptr<common_utils::logging::Logger> logger)
            : metadataProvider_(std::move(metadataProvider)), 
              timeVariableName_(timeVariableName),
              logger_(std::move(logger)),
              calendarConverter_(std::make_shared<common_utils::time::CalendarConverter>()) {
            logger_->info("CFTimeExtractor created for variable: {}", timeVariableName_);
        }

        boost::future<std::optional<CFTimeExtractor::TimeAttrs>> CFTimeExtractor::fetchTimeAttributesAsync() {
            auto provider = metadataProvider_.lock();
            if (!provider) {
                logger_->warn("MetadataProvider expired for CFTimeExtractor on var: {}", timeVariableName_);
                return boost::make_ready_future<std::optional<TimeAttrs>>(std::nullopt);
            }

            return provider->getVariableMetadataAsync(timeVariableName_)
                .then(boost::launch::deferred, [this](boost::future<std::optional<std::vector<MetadataEntry>>> futureAttrs) {
                    auto variableAttrsOpt = futureAttrs.get();
                    if (!variableAttrsOpt) {
                        logger_->warn("Could not fetch metadata for time variable: {}", timeVariableName_);
                        return std::optional<TimeAttrs>(std::nullopt);
                    }
                    return parseTimeAttributes(*variableAttrsOpt);
                });
        }

        std::optional<CFTimeExtractor::TimeAttrs> CFTimeExtractor::parseTimeAttributes(
            const std::vector<MetadataEntry>& attrs) {
            TimeAttrs parsedAttrs;
            std::string calendarStr = "standard"; // CF Default

            for (const auto& entry : attrs) {
                if (entry.key == "units") {
                    parsedAttrs.units = entry.value;
                } else if (entry.key == "calendar") {
                    calendarStr = common_utils::string::toLowerCase(entry.value);
                }
            }

            if (parsedAttrs.units.empty()) {
                logger_->warn("Time variable '{}' has no 'units' attribute.", timeVariableName_);
                return std::nullopt;
            }

            parsedAttrs.calendar = common_utils::time::stringToCalendarType(calendarStr);
            
            // Use common_utils::time::CFTimeParser to parse units string
            auto parseResult = common_utils::time::CFTimeParser::parseCFUnits(parsedAttrs.units, parsedAttrs.calendar);
            if (!parseResult) {
                 logger_->warn("Failed to parse CF units string '{}' for variable '{}'", parsedAttrs.units, timeVariableName_);
                 return std::nullopt;
            }
            parsedAttrs.referenceDateTime = parseResult->referenceTimestamp;
            // Store also the unit multiplier (e.g., seconds, days) from parseResult if needed for conversion logic

            logger_->debug("Parsed time attributes for '{}': units='{}', calendar='{}'", 
                timeVariableName_, parsedAttrs.units, calendarStr);
            return parsedAttrs;
        }

        // Helper to get (and cache) time attributes
        // boost::shared_future<std::optional<CFTimeExtractor::TimeAttrs>> getCachedTimeAttrs(CFTimeExtractor* self) {
        //      boost::unique_lock<boost::shared_mutex> lock(self->timeAttrsMutex_);
        //      if (!self->timeAttrsFuture_.is_ready()) { // Check if already fetching or fetched
        //          self->timeAttrsFuture_ = self->fetchTimeAttributesAsync().share();
        //      }
        //      return self->timeAttrsFuture_;
        // }
        // Corrected getCachedTimeAttrs to be a member function for easier access to member variables
        boost::shared_future<std::optional<CFTimeExtractor::TimeAttrs>> CFTimeExtractor::getCachedTimeAttrs() {
             boost::unique_lock<boost::shared_mutex> lock(timeAttrsMutex_); // Protects concurrent initialization
             if (!timeAttrsFuture_.is_valid() || !timeAttrsFuture_.is_ready()) { // is_valid() to check if it holds a shared state
                 timeAttrsFuture_ = fetchTimeAttributesAsync().share();
             }
             return timeAttrsFuture_;
        }


        boost::future<std::optional<common_utils::time::CalendarTime>> 
        CFTimeExtractor::convertCoordinateToCalendarTimeAsync(double timeCoordinateValue) {
            return getCachedTimeAttrs().then(boost::launch::deferred, 
                [this, timeCoordinateValue](boost::shared_future<std::optional<TimeAttrs>> futureCachedAttrs) {
                auto attrsOpt = futureCachedAttrs.get();
                if (!attrsOpt || !attrsOpt.has_value()) {
                    logger_->warn("Cannot convert time coordinate for '{}': missing attributes.", timeVariableName_);
                    return std::optional<common_utils::time::CalendarTime>(std::nullopt);
                }
                const auto& attrs = attrsOpt.value();
                
                // Actual conversion using CalendarConverter
                return calendarConverter_->convertCFTimeToCalendarTime(timeCoordinateValue, attrs.units, attrs.calendar);
            });
        }

        boost::future<std::vector<std::optional<common_utils::time::CalendarTime>>>
        CFTimeExtractor::convertCoordinatesToCalendarTimesAsync(const std::vector<double>& timeCoordinateValues) {
            return getCachedTimeAttrs().then(boost::launch::deferred, 
                [this, timeCoordinateValues](boost::shared_future<std::optional<TimeAttrs>> futureCachedAttrs) {
                auto attrsOpt = futureCachedAttrs.get();
                if (!attrsOpt || !attrsOpt.has_value()) {
                    logger_->warn("Cannot convert time coordinates for '{}': missing attributes.", timeVariableName_);
                    return std::vector<std::optional<common_utils::time::CalendarTime>>(timeCoordinateValues.size(), std::nullopt);
                }
                const auto& attrs = attrsOpt.value();
                std::vector<std::optional<common_utils::time::CalendarTime>> results;
                results.reserve(timeCoordinateValues.size());
                for (double val : timeCoordinateValues) {
                     results.push_back(calendarConverter_->convertCFTimeToCalendarTime(val, attrs.units, attrs.calendar));
                }
                return results;
            });
        }

        boost::future<std::optional<common_utils::time::TimeRange>> CFTimeExtractor::getTimeRangeAsync() {
            logger_->warn("getTimeRangeAsync for CFTimeExtractor is not fully implemented yet for '{}'. Needs coordinate data access or IMetadataProvider enhancement.", timeVariableName_);
            return boost::make_ready_future<std::optional<common_utils::time::TimeRange>>(std::nullopt);
        }

} // namespace
```
        *注: `CFTimeExtractor` 的实现依赖 `common_utils::time` 中强大的 `CalendarConverter` 和一个理想的 `CFTimeParser`。`getTimeRangeAsync` 的实现比较复杂，可能需要 `IDataProvider` 的能力来读取时间轴的实际数值范围，这超出了纯元数据提取的范畴，需要仔细考虑其职责划分。`getCachedTimeAttrs`辅助函数是为了避免重复异步获取元数据属性。*

2.  **在 NetCDF 读取器中使用 `CFTimeExtractor`** (将在Phase 5具体实现 `NetCDFUnifiedReader` 时集成):
    *   当 `NetCDFUnifiedReader` (未来的) 需要获取时间范围或转换时间坐标时，它会创建 `CFTimeExtractor` 的实例。
```cpp
        // In NetCDFUnifiedReader.cpp (Conceptual for Phase 5)
        // ...
        // private:
        //  std::map<std::string, std::shared_ptr<time::CFTimeExtractor>> timeExtractors_;
        // ...
        // boost::future<std::optional<TimeRange>> NetCDFUnifiedReader::getTimeRangeAsync(
        //     const std::string& timeVariableName) const {
        //     
        //     auto it = timeExtractors_.find(timeVariableName);
        //     if (it == timeExtractors_.end()) {
        //         // Create on demand, passing 'this' as IMetadataProvider (if reader implements it)
        //         auto extractor = std::make_shared<time::CFTimeExtractor>(
        //              std::static_pointer_cast<api::IMetadataProvider>(shared_from_this()), 
        //              timeVariableName, 
        //              logger_);
        //         timeExtractors_[timeVariableName] = extractor; // Cache it
        //         it = timeExtractors_.find(timeVariableName);
        //     }
        //     return it->second->getTimeRangeAsync(); // This still has the issue of needing data
        // }
        ```
    *   当读取具体的时间坐标数据后，可以使用 `CFTimeExtractor::convertCoordinatesToCalendarTimesAsync` 进行转换。

3.  **删除旧的时间处理代码**:
    *   `src/impl/readers/netcdf/parsing/netcdf_time_processor.h`
    *   `src/impl/readers/netcdf/parsing/netcdf_time_processor.cpp`
    *   从 `netcdf_coordinate_system_parser.cpp` (或类似文件) 中移除所有与时间单位解析、时间值转换相关的旧逻辑，使其只关注空间坐标系统和维度。
    *   更新 CMakeLists.txt。

4.  **编写单元测试**: (`tests/data_access/time/cf_time_extractor_tests.cpp`)
    *   **Mock `IMetadataProvider`**: 用于提供各种时间变量的 `units` 和 `calendar` 属性组合。
    *   测试 `convertCoordinateToCalendarTimeAsync`:
        *   不同单位: "seconds since ...", "days since ...", "hours since ...", etc.
        *   不同参考日期和时间。
        *   不同日历: "standard", "gregorian", "proleptic_gregorian", "noleap", "360_day", "365_day", "366_day", etc.
        *   边界条件和无效输入（如错误的单位字符串、不支持的日历）。
    *   测试 `convertCoordinatesToCalendarTimesAsync` 的批量转换正确性。
    *   (如果实现) 测试 `getTimeRangeAsync` 的逻辑，但这可能需要更复杂的 Mock 或将其作为集成测试的一部分。

5.  **集成与验证**: 
    *   在NetCDF读取相关的集成测试中，验证通过 `CFTimeExtractor` 获取的时间信息（如时间范围、具体时间点）是否与已知测试文件中的内容一致。
    *   特别关注不同日历类型下的日期计算准确性。

**涉及文件 (Phase 4):**
*   **新增/实现**: 
    *   `src/time/cf_time_extractor.h`
    *   `src/time/cf_time_extractor.cpp`
    *   `tests/data_access/time/cf_time_extractor_tests.cpp`
*   **修改**: 
    *   `src/readers/netcdf/internal/netcdf_coordinate_system_parser.cpp` (或类似文件): 移除旧的时间处理逻辑。
    *   (在Phase 5) `src/readers/netcdf/netcdf_unified_reader.cpp` (将集成 `CFTimeExtractor`)
    *   CMakeLists.txt (添加新文件，移除旧文件)
*   **删除**: 
    *   `src/impl/readers/netcdf/parsing/netcdf_time_processor.h`
    *   `src/impl/readers/netcdf/parsing/netcdf_time_processor.cpp`

**风险控制与验证 (Phase 4):**
*   **`common_utils::time` 依赖**: `CFTimeExtractor` 的健壮性和准确性高度依赖 `common_utils::time` 中 `CalendarConverter` 和 `CFTimeParser` 的正确性。确保这些基础组件经过充分测试。
*   **CF约定覆盖**: CF约定对于时间的表示有很多细节。确保 `CFTimeExtractor` 和依赖的 `common_utils` 组件能覆盖常见的和一些不常见的但有效的CF时间表示。
*   **`IMetadataProvider` 接口**: `CFTimeExtractor` 需要从 `IMetadataProvider` 获取时间变量的属性。确保此交互清晰且高效（如通过 `shared_future` 缓存属性）。
*   **`getTimeRangeAsync` 职责**: 如前述，获取完整时间范围可能需要数据访问权限，需要仔细考虑此方法在 `CFTimeExtractor` 中的定位和实现方式，或者将其移至更高层级。
*   **测试数据**: 需要准备包含各种CF时间单位和日历类型的NetCDF测试文件，以进行有效的单元测试和集成测试。

**预期成果 (Phase 4):**
1.  一个功能完善、经过测试的 `CFTimeExtractor`，能够准确解析和转换CF约定的时间信息。
2.  移除了约 200+ 行特定于NetCDF的旧时间处理代码。
3.  时间处理逻辑更加集中、标准化，并与 `common_utils` 保持一致。
4.  为Phase 5中NetCDF读取器的重构做好了时间处理方面的准备。
5.  通过测试验证了时间转换在各种常见CF约定下的准确性。

---
*这是Phase 4的详细计划。请您审阅。确认后，我将继续提供Phase 5（读取器架构重构）的计划。*

---

### Phase 5: 读取器架构重构 (预计 Week 5-6)

**🎯 目标:**
-   建立一个基于新接口 (`IDataSource`, `IMetadataProvider`, `IDataProvider`, `IStreamingDataProvider`) 的统一读取器架构。
-   实现 `UnifiedDataReader` 抽象基类，为各种具体文件格式的读取器提供通用功能和统一的实现模式。
-   将现有的 GDAL 和 NetCDF 读取逻辑重构为继承自 `UnifiedDataReader` 的具体实现类 (如 `GdalUnifiedReader`, `NetCDFUnifiedReader`)。
-   实现 `ReaderRegistry` (读取器注册表) 和 `FormatDetector` (格式探测器)，以支持根据文件类型/内容动态创建合适的读取器实例。
-   彻底移除旧的 `IDataReaderImpl` 接口、`SharedReaderVariant` 以及相关的 `ReaderFactory`。
-   使 `DataAccessServiceImpl` 通过 `ReaderRegistry` 获取读取器实例，而不是直接实例化或使用旧工厂。

**核心任务:**
1.  **实现 `UnifiedDataReader` 基类**: 包含对 `IDataSource` 部分接口的通用实现，并声明其他接口的纯虚函数，供子类实现。
2.  **实现 `FormatDetector`**: 设计 `IFormatDetector` 接口及 `FormatDetectorImpl` 实现，能够根据文件扩展名、文件头部魔法数等信息判断文件格式。
3.  **实现 `ReaderRegistry`**: 能够注册不同格式的读取器工厂函数，并根据 `FormatDetector` 的结果创建相应的 `UnifiedDataReader` 实例。
4.  **重构 GDAL 读取器**: 创建 `GdalUnifiedReader` (可能需要区分 Raster 和 Vector，或用一个类处理两种情况)，实现所有新接口，内部封装GDAL库的调用。将原 `gdal_raster_reader.h/cpp` 和 `gdal_vector_reader.h/cpp` 的逻辑迁移并重构到这里。
5.  **重构 NetCDF 读取器**: 创建 `NetCDFUnifiedReader`，实现所有新接口，内部封装NetCDF库调用，并使用 Phase 4 创建的 `CFTimeExtractor` 处理时间。将原 `netcdf_cf_reader.h/cpp` 的逻辑迁移并重构。
6.  **重构 `DataAccessServiceImpl`**: 修改其获取和使用数据读取器的方式，使其通过 `ReaderRegistry` 来动态创建读取器实例。
7.  **删除旧的读取器相关代码**: 包括 `IDataReaderImpl`, `SharedReaderVariant`, `ReaderFactory` 以及旧的读取器实现文件。
8.  **编写全面的单元测试和集成测试**: 针对新的读取器基类、具体实现、注册表、格式探测器进行测试。

**详细步骤与代码实现指引:**

1.  **创建 `UnifiedDataReader` 基类**: (`src/readers/core/`)
    *   `unified_data_reader.h`:
```cpp
#pragma once
#include <string>
        #include <memory>
#include <boost/future.hpp>
        #include "../../api/i_data_source.h"
        #include "../../api/i_metadata_provider.h"
        #include "../../api/i_data_provider.h"
        #include "../../api/i_streaming_data_provider.h"
        #include "common_utils/logging/logger.h"
        #include "core_services/crs/i_crs_service.h" // 依赖CRS服务进行坐标转换

namespace oscean::core_services::data_access::readers {

        class UnifiedDataReader : public api::IDataSource,
                                  public api::IMetadataProvider,
                                  public api::IDataProvider,
                                  public api::IStreamingDataProvider,
                                  public std::enable_shared_from_this<UnifiedDataReader> { // enable_shared_from_this for async callbacks
        protected:
            std::string filePath_;
            bool isOpen_ = false;
            std::shared_ptr<common_utils::logging::Logger> logger_;
            std::shared_ptr<core_services::ICrsService> crsService_; // Injected CRS service
            // UnifiedAsyncExecutor might be needed here if reader operations are complex and need offloading
            // std::shared_ptr<async::UnifiedAsyncExecutor> asyncExecutor_;
        
public:
            UnifiedDataReader(
                const std::string& filePath,
        std::shared_ptr<common_utils::logging::Logger> logger,
                std::shared_ptr<core_services::ICrsService> crsService
                // std::shared_ptr<async::UnifiedAsyncExecutor> asyncExecutor // Optional injection
            );
            virtual ~UnifiedDataReader() = default;
        
            // IDataSource - 部分默认实现
            std::string getFilePath() const noexcept override { return filePath_; }
            bool isOpen() const noexcept override { return isOpen_; }
        
            // IDataSource - 子类必须实现
            boost::future<bool> openAsync(const std::string& filePath) override = 0;
            boost::future<void> closeAsync() override = 0;
            std::string getDataSourceType() const noexcept override = 0; // e.g., "GDAL_RASTER", "NETCDF_CF"
            boost::future<std::optional<FileMetadata>> extractFileMetadataSummaryAsync() override = 0; // For caching/format detection aid

            // IMetadataProvider - 子类必须实现所有方法
            // ... (getVariableNamesAsync, getNativeCrsAsync, etc.)
        
            // IDataProvider - 子类必须实现所有方法

            ---

### Phase 6: 流式处理实现与最终集成 (预计 Week 7)

**🎯 目标:**
-   在新的读取器架构基础上，为 `DataAccessService` 添加流式数据读取能力，允许客户端逐块处理大规模数据集，以控制内存占用。
-   实现 `StreamingDataProcessor`，负责协调流式读取操作，管理数据块的获取和分发。
-   确保 `GdalUnifiedReader` 和 `NetCDFUnifiedReader` 正确并高效地实现 `IStreamingDataProvider` 接口。
-   完成所有组件的最终集成，进行全面的回归测试、性能测试和压力测试。
-   编写最终的重构报告和更新相关文档，确认所有重构目标均已达成。

**核心任务:**
1.  **完善 `IStreamingDataProvider` 接口实现**: 在 `GdalUnifiedReader` 和 `NetCDFUnifiedReader` 中具体实现 `streamVariableDataAsync` 和/或 `getNextDataChunkAsync` 方法，确保能够有效地从底层库逐块读取数据。
2.  **实现 `StreamingDataProcessor`**: 该类将使用 `ReaderRegistry` 获取实现了 `IStreamingDataProvider` 接口的读取器，并管理流式数据块的处理流程。
3.  **在 `DataAccessServiceImpl` 中暴露流式API**: 添加新的公共方法到 `IDataAccessService` 和 `DataAccessServiceImpl`，允许客户端发起流式读取请求，并接收数据块流。
4.  **编写流式处理的单元测试和集成测试**: 专注于大数据集的流式读取、内存控制、背压机制（如果实现）、错误处理和取消操作。
5.  **进行系统级集成与回归测试**: 确保所有重构后的组件（缓存、异步、时间、读取器、流式处理）协同工作正常，且所有原有功能未受影响。
6.  **执行最终性能和压力测试**: 对比重构前后的性能指标，验证是否达到预期目标（缓存命中率、内存使用、响应时间等）。
7.  **代码审查与文档更新**: 对所有主要的新代码和重构代码进行最终审查。更新所有相关的设计文档、API文档和用户手册。

**详细步骤与代码实现指引:**

1.  **完善 `GdalUnifiedReader` 和 `NetCDFUnifiedReader` 中的 `IStreamingDataProvider` 实现**:
    *   **`streamVariableDataAsync` 方法**:
        *   内部循环读取数据块 (e.g., using GDAL\'s `RasterIO` with windowing, or NetCDF\'s `get_vara` for hyperslabs)。
        *   每读取一个数据块 (`DataChunk`)，就调用传递进来的 `chunkProcessor` 回调函数。
        *   处理回调函数的 `boost::future<bool>` 返回值，如果为 `false`，则提前中止流式处理。
        *   需要仔细管理读取的偏移量/索引，确保不遗漏、不重复。
        *   考虑合适的默认块大小，并允许通过 `StreamingOptions` 进行配置。
    *   **`getNextDataChunkAsync` 方法 (如果选择实现pull模式)**:
        *   读取器内部需要维护当前流的状态（如当前块索引、文件句柄）。
        *   每次调用返回下一个数据块，如果到达数据末尾则返回 `std::nullopt`。
    *   **错误处理**: 确保在读取过程中发生的任何I/O错误或库错误都能被正确捕获并向上传播。
    *   **资源管理**: 确保在流式处理结束或发生错误时，所有打开的资源都得到释放。

2.  **创建 `StreamingDataProcessor`**: (`src/streaming/`)
    *   `streaming_data_processor.h`:
```cpp
#pragma once
#include <memory>
#include <functional>
        #include <string>
#include <boost/future.hpp>
        #include "../api/i_streaming_data_provider.h" // For DataChunk
        #include "../api/streaming_types.h"
        #include "../async/unified_async_executor.h"
        #include "../readers/core/reader_registry.h"
        #include "common_utils/logging/logger.h"

namespace oscean::core_services::data_access::streaming {

class StreamingDataProcessor {
public:
    StreamingDataProcessor(
        std::shared_ptr<readers::ReaderRegistry> readerRegistry,
                std::shared_ptr<async::UnifiedAsyncExecutor> asyncExecutor, // For managing reader creation & stream operations
        std::shared_ptr<common_utils::logging::Logger> logger);
    
            // Initiates and manages a push-based stream of data chunks
    boost::future<void> processStreamAsync(
        const std::string& filePath,
        const std::string& variableName,
                const api::StreamingOptions& options,
                std::function<boost::future<bool>(api::DataChunk)> chunkProcessor);

            // Potentially: methods for pull-based streaming if getNextDataChunkAsync is implemented
            // boost::future<std::string> openStreamAsync(const std::string& filePath, ...); // Returns a stream handle
            // boost::future<std::optional<api::DataChunk>> pullNextChunkAsync(const std::string& streamHandle);
            // boost::future<void> closeStreamAsync(const std::string& streamHandle);
    
private:
    std::shared_ptr<readers::ReaderRegistry> readerRegistry_;
            std::shared_ptr<async::UnifiedAsyncExecutor> asyncExecutor_;
    std::shared_ptr<common_utils::logging::Logger> logger_;
    
            // Helper to get a reader that supports streaming
            boost::future<std::shared_ptr<api::IStreamingDataProvider>> getStreamingReaderAsync(
                const std::string& filePath);
        };

        } // namespace
        ```
    *   `streaming_data_processor.cpp`:
        ```cpp
        #include "streaming_data_processor.h"
        #include "../readers/core/unified_data_reader.h"

        namespace oscean::core_services::data_access::streaming {

        StreamingDataProcessor::StreamingDataProcessor(
            std::shared_ptr<readers::ReaderRegistry> readerRegistry,
            std::shared_ptr<async::UnifiedAsyncExecutor> asyncExecutor,
            std::shared_ptr<common_utils::logging::Logger> logger)
            : readerRegistry_(std::move(readerRegistry)),
              asyncExecutor_(std::move(asyncExecutor)),
              logger_(std::move(logger)) {
            logger_->info("StreamingDataProcessor initialized.");
        }

        boost::future<std::shared_ptr<api::IStreamingDataProvider>>
        StreamingDataProcessor::getStreamingReaderAsync(const std::string& filePath) {
            return readerRegistry_->createReaderAsync(filePath)
                .then(boost::launch::deferred, [this, filePath](boost::future<std::shared_ptr<readers::UnifiedDataReader>> futureReader) {
                    auto reader = futureReader.get();
                    if (!reader) {
                        logger_->error("Failed to create reader for file: {}", filePath);
                        throw std::runtime_error("Failed to create reader for streaming: " + filePath);
                    }
                    // Attempt to cast to IStreamingDataProvider
                    std::shared_ptr<api::IStreamingDataProvider> streamingProvider =
                        std::dynamic_pointer_cast<api::IStreamingDataProvider>(reader);
                    if (!streamingProvider) {
                        logger_->error("Reader for file \'{}\' does not support IStreamingDataProvider.", filePath);
                        // reader->closeAsync(); // Important to close if not usable
                        throw std::runtime_error("Reader does not support streaming: " + filePath);
                    }
                    return streamingProvider;
                });
        }

        boost::future<void> StreamingDataProcessor::processStreamAsync(
            const std::string& filePath,
        const std::string& variableName,
            const api::StreamingOptions& options,
            std::function<boost::future<bool>(api::DataChunk)> chunkProcessor) {

            return getStreamingReaderAsync(filePath)
                .then(boost::launch::deferred,
                    [this, variableName, options, chunkProcessor](boost::future<std::shared_ptr<api::IStreamingDataProvider>> futureProvider) {
                    auto streamingProvider = futureProvider.get();
                    // Ensure the reader (IDataSource part) is open before streaming
                    // This might be handled by createReaderAsync or needs explicit call
                    // For simplicity, assuming createReaderAsync already opens or it handles it internally.

                    // Submit the actual streaming operation via asyncExecutor if it's long-running
                    // or if the IStreamingDataProvider::streamVariableDataAsync itself is fully async.
                    // Here we assume streamVariableDataAsync is a boost::future returning method.
                    return streamingProvider->streamVariableDataAsync(variableName, chunkProcessor, options);
                })
                .then(boost::launch::deferred, [this, filePath](boost::future<void> streamFuture) {
                    try {
                        streamFuture.get(); // Propagate exceptions
                        logger_->info("Successfully completed streaming for file: {}", filePath);
                    } catch (const std::exception& e) {
                        logger_->error("Error during streaming for file \'{}\': {}", filePath, e.what());
                        // Reader should be closed by its own RAII or explicit close in IStreamingDataProvider impl
                        throw; // Re-throw to propagate
                    }
                    // Close the reader? If the reader was cached, this might be an issue.
                    // Lifecycle of reader needs care. If obtained from registry and cached,
                    // closing it here might affect other users of the cached reader.
                    // Best if IStreamingDataProvider methods handle their open/close internally for the stream,
                    // or if the reader is not cached for streaming operations, or ref-counted.
                });
        }

} // namespace
```

3.  **在 `DataAccessServiceImpl` 中暴露流式 API**: (`src/`)
    *   修改 `include/core_services/data_access/i_data_access_service.h`: 添加新的流式接口，例如：
        ```cpp
        virtual boost::future<void> streamGridVariableAsync(
            const std::string& filePath,
            const std::string& variableName,
            const api::StreamingOptions& options,
            std::function<boost::future<bool>(api::DataChunk)> chunkProcessor) = 0;
        ```
    *   在 `data_access_service_impl.h/cpp` 中实现此接口：
        *   注入 `StreamingDataProcessor` 实例（或在构造函数中创建）。
        *   实现方法将调用 `streamingDataProcessor_->processStreamAsync(...)`。
        ```cpp
        // data_access_service_impl.h
        // ... private members ...
        // std::shared_ptr<streaming::StreamingDataProcessor> streamingProcessor_;

        // data_access_service_impl.cpp - constructor
        // streamingProcessor_ = std::make_shared<streaming::StreamingDataProcessor>(readerRegistry_, asyncExecutor_, logger_);

        // data_access_service_impl.cpp - method impl
        boost::future<void> DataAccessServiceImpl::streamGridVariableAsync(
            const std::string& filePath,
            const std::string& variableName,
            const api::StreamingOptions& options,
            std::function<boost::future<bool>(api::DataChunk)> chunkProcessor) {
            if (!streamingProcessor_) { // Should have been initialized
                return boost::make_exceptional_future<void>(std::runtime_error("StreamingProcessor not initialized"));
            }
            return streamingProcessor_->processStreamAsync(filePath, variableName, options, chunkProcessor);
        }
        ```

4.  **流式处理的单元测试和集成测试**:
    *   **单元测试**:
        *   为 `GdalUnifiedReader` 和 `NetCDFUnifiedReader` 的流式方法编写单元测试，Mock底层库调用，验证数据块的正确分割和回调调用。
        *   为 `StreamingDataProcessor` 编写单元测试，Mock `ReaderRegistry` 和 `IStreamingDataProvider`，测试其协调逻辑。
    *   **集成测试**: (`tests/data_access/streaming_tests.cpp`)
        *   使用真实的、非常大的栅格和NetCDF文件。
        *   验证流式读取是否能完整读取所有数据，且顺序正确。
        *   监控测试过程中的内存使用，确保其保持在较低且稳定的水平。
        *   测试不同的 `StreamingOptions` (如不同的块大小)。
        *   测试当 `chunkProcessor` 返回 `false` 时，流是否能正确中止。
        *   测试网络断开、文件损坏等错误情况下的行为。

5.  **系统级集成与回归测试**:
    *   执行项目中的所有现有单元测试和集成测试，确保没有功能回归。
    *   手动测试或通过自动化UI/API测试（如果适用）来验证系统的关键路径。

6.  **最终性能和压力测试**:
    *   重新运行 Phase 1 建立的性能基准测试，并增加针对流式API的性能测试。
    *   对比重构前后的关键性能指标 (缓存命中率、内存利用率、特定操作的响应时间、大数据文件处理时间)。
    *   进行压力测试，模拟高并发请求，观察系统的稳定性和资源消耗。

7.  **代码审查与文档更新**:
    *   组织对 Phase 1-6 中所有主要新增和修改代码的全面审查。
    *   更新 `数据处理重构方案.MD` 以反映最终的实现决策和遇到的问题。
    *   更新模块的 README、API 文档 (Doxygen 注释)、用户指南等。
    *   编写一份总结性的重构报告，包括目标达成情况、遇到的挑战、学到的经验教训。

**涉及文件 (Phase 6):**
*   **新增/实现**:
    *   `src/streaming/streaming_data_processor.h/cpp`
    *   `tests/data_access/streaming_tests.cpp`
*   **修改**:
    *   `src/readers/gdal/gdal_unified_reader.h/cpp` (实现 `IStreamingDataProvider`)
    *   `src/readers/netcdf/netcdf_unified_reader.h/cpp` (实现 `IStreamingDataProvider`)
    *   `include/core_services/data_access/i_data_access_service.h` (添加流式API)
    *   `src/data_access_service_impl.h/cpp` (实现流式API, 注入 `StreamingDataProcessor`)
    *   CMakeLists.txt (添加新文件)
    *   所有相关测试文件以覆盖新功能。
*   **删除**: 无特定文件删除，主要是完善和集成。

**风险控制与验证 (Phase 6):**
*   **流式实现的复杂性**: 正确实现逐块读取、状态管理、错误处理和资源释放可能很复杂，尤其是在涉及回调和异步操作时。
*   **性能开销**: 流式处理本身可能有一定的性能开销（如多次小I/O代替一次大I/O）。需要平衡内存控制和吞吐量。
*   **背压机制**: 如果客户端处理数据块的速度跟不上数据块生成的速度，可能会导致内存积压。一个完整的流式系统可能需要背压机制 (backpressure)，这会增加复杂性 (初期可简化，不实现复杂背压)。
*   **读取器生命周期**: 在 `StreamingDataProcessor` 中获取和使用读取器时，要注意其生命周期管理，特别是当读取器实例被缓存时。流式操作可能需要独占或特殊处理读取器实例。
*   **全面测试的难度**: 测试大规模数据流的各种边界条件和错误情况可能很有挑战性。

core_services_impl/data_access_service/
├── include/core_services/data_access/  # 公共接口和核心类型 (对外暴露)
│   ├── i_data_access_service.h         # (保持不变) 服务主接口
│   ├── common_types.h                  # (新增/重构) 模块特定的公共类型定义 (如 DataChunkKey, FileMetadata, BoundingBox, TimeRange, DimensionDefinition 等)
│   ├── error_codes.h                   # (新增) 模块特定的错误码定义
│   └── api/                              # (新增) 新的核心接口定义
│       ├── i_data_source.h
│       ├── i_metadata_provider.h
│       ├── i_data_provider.h
│       ├── i_streaming_data_provider.h
│       ├── data_access_requests.h      # (GridReadRequest, FeatureReadRequest, etc.)
│       ├── data_access_responses.h     # (GridData, FeatureCollection, DataChunk etc.)
│       └── streaming_types.h           # (StreamingOptions, BackpressureControl etc.)
│
└── src/                                  # 实现代码 (内部细节)
    ├── data_access_service_impl.h        # (重命名/重构自 raw_data_access_service_impl.h)
    ├── data_access_service_impl.cpp      # (重命名/重构自 raw_data_access_service_impl.cpp)
    ├── cache/                              # (新增) 统一缓存实现
    │   ├── unified_data_access_cache.h
    │   ├── unified_data_access_cache.cpp
    │   └── i_cacheable_value.h           # (新增) 可缓存对象接口
    ├── async/                              # (新增) 统一异步执行器
    │   ├── unified_async_executor.h
    │   └── unified_async_executor.cpp
    ├── time/                               # (新增) 时间处理相关
    │   ├── cf_time_extractor.h
    │   └── cf_time_extractor.cpp
    ├── readers/                            # (重构) 读取器相关实现
    │   ├── core/                           # (新增) 读取器核心组件
    │   │   ├── unified_data_reader.h     # 抽象基类
    │   │   ├── unified_data_reader.cpp
    │   │   ├── reader_registry.h
    │   │   ├── reader_registry.cpp
    │   │   ├── i_format_detector.h       # 格式探测器接口
    │   │   ├── format_detector_impl.h    # 具体格式探测器实现
    │   │   └── format_detector_impl.cpp
    │   ├── gdal/                           # (重构) GDAL 读取器实现
    │   │   ├── gdal_unified_reader.h       # 继承 UnifiedDataReader
    │   │   ├── gdal_unified_reader.cpp
    │   │   └── internal/                   # GDAL内部辅助组件 (保持封装, 内容重构)
    │   │       ├── gdal_dataset_handler.h/cpp
    │   │       ├── gdal_metadata_extractor.h/cpp (各类元数据提取器)
    │   │       ├── gdal_raster_io.h/cpp
    │   │       ├── gdal_vector_io.h/cpp
    │   │       └── utils/ (gdal_common_utils, gdal_type_conversion etc.)
    │   └── netcdf/                         # (重构) NetCDF 读取器实现
    │       ├── netcdf_unified_reader.h     # 继承 UnifiedDataReader
    │       ├── netcdf_unified_reader.cpp
    │       └── internal/                   # NetCDF内部辅助组件 (保持封装, 内容重构)
    │           ├── netcdf_file_processor.h/cpp
    │           ├── netcdf_metadata_manager.h/cpp
    │           ├── io/ (netcdf_attribute_io, netcdf_variable_io etc.)
    │           └── parsing/ (netcdf_cf_conventions, netcdf_coordinate_system_parser (精简后) etc.)
    └── streaming/                          # (新增) 流式处理实现
        ├── streaming_data_processor.h
        └── streaming_data_processor.cpp

以下是原文件目录架构以及重构中的去向方案。
        core_services_impl/data_access_service/
├── include/core_services/data_access/  # 对外公共头文件目录
│   ├── i_data_access_service.h         # 🔒 保留 (适配): 接口本身不变，但其实现类会被彻底重构。新增流式API。
│   │                                   #    新位置: include/core_services/data_access/i_data_access_service.h
│   ├── boost_future_config.h           # ➡️ 删除 (Phase 3): 被统一的异步执行器 UnifiedAsyncExecutor 取代，不再需要特定配置。
│   ├── readers/                        # (旧的读取器相关公共头文件子目录)
│   │   └── data_reader_common.h        # ➡️ 删除 (Phase 2/5): 其内容（如通用枚举、简单结构体）可能部分迁移至新的 `common_types.h` 或被新API结构取代。
│   └── i_data_reader_impl.h            # ➡️ 删除 (Phase 5): 被新的API接口 (`IDataSource`, `IMetadataProvider`, `IDataProvider`, `IStreamingDataProvider`) 和 `UnifiedDataReader` 基类彻底取代。
│   └── (其他可能的旧公共头文件)          # 🔗 迁移/合并逻辑至: 根据具体内容，或被删除，或其定义迁移到 `common_types.h`, `error_codes.h` 或新的API接口文件中。
│
└── src/                                  # 源代码实现目录
    ├── impl/                             # (通常包含大量的旧实现细节)
    │   ├── raw_data_access_service_impl.h  # 🔄 重命名与重构至: src/data_access_service_impl.h (Phase 2-6) - 作为服务主实现类，但内部逻辑完全重写。
    │   ├── raw_data_access_service_impl.cpp# 🔄 重命名与重构至: src/data_access_service_impl.cpp (Phase 2-6) - 同上。
    │   │
    │   ├── cache/                          # ➡️ 删除 (整个目录及其下所有文件在 Phase 2 被移除)
    │   │   ├── data_chunk_cache.h          # ➡️ 删除: 被 src/cache/unified_data_access_cache.h/cpp 取代。
    │   │   ├── data_chunk_cache.cpp        # ➡️ 删除: 同上。
    │   │   ├── reader_cache.h              # ➡️ 删除: 被 src/cache/unified_data_access_cache.h/cpp (用于缓存读取器实例) 取代。
    │   │   ├── reader_cache.cpp            # ➡️ 删除: 同上。
    │   │   ├── metadata_cache.h            # ➡️ 删除: 被 src/cache/unified_data_access_cache.h/cpp (用于缓存元数据) 取代。
    │   │   ├── metadata_cache.cpp          # ➡️ 删除: 同上。
    │   │   ├── netcdf_cache_manager.h      # ➡️ 删除: NetCDF特定缓存逻辑统一到新缓存框架中。
    │   │   ├── netcdf_cache_manager.cpp    # ➡️ 删除: 同上。
    │   │   └── cache_manager_template.h    # ➡️ 删除: 通用缓存模板被 `common_utils::cache` 和 `UnifiedDataAccessCache` 取代。
    │   │
    │   ├── factory/                        # ➡️ 删除 (整个目录及其下所有文件在 Phase 5 被移除)
    │   │   ├── reader_factory.h            # ➡️ 删除: 被 src/readers/core/reader_registry.h/cpp 和 src/readers/core/format_detector_impl.h/cpp 取代。
    │   │   └── reader_factory.cpp          # ➡️ 删除: 同上。
    │   │
    │   ├── readers/                        # (旧的具体读取器实现子目录)
    │   │   ├── gdal/                       # (GDAL 相关旧实现)
    │   │   │   ├── gdal_raster_reader.h    # 🔗 迁移/合并逻辑至: src/readers/gdal/gdal_unified_reader.h/cpp (Phase 5). 逻辑被重构以适配新接口。
    │   │   │   ├── gdal_raster_reader.cpp  # 🔗 迁移/合并逻辑至: src/readers/gdal/gdal_unified_reader.h/cpp (Phase 5).
    │   │   │   ├── gdal_vector_reader.h    # 🔗 迁移/合并逻辑至: src/readers/gdal/gdal_unified_reader.h/cpp (Phase 5). (可能与RasterReader合并或共享内部组件).
    │   │   │   ├── gdal_vector_reader.cpp  # 🔗 迁移/合并逻辑至: src/readers/gdal/gdal_unified_reader.h/cpp (Phase 5).
    │   │   │   ├── io/                     # (旧的GDAL IO辅助组件)
    │   │   │   │   └── (e.g., gdal_raster_io_utils.h/cpp) # 🔗 迁移/合并逻辑至: src/readers/gdal/internal/gdal_raster_io.h/cpp 或直接整合进 gdal_unified_reader.cpp.
    │   │   │   ├── metadata/               # (旧的GDAL元数据提取辅助组件)
    │   │   │   │   └── (e.g., gdal_attribute_reader.h/cpp) # 🔗 迁移/合并逻辑至: src/readers/gdal/internal/gdal_metadata_extractor.h/cpp 或直接整合进 gdal_unified_reader.cpp.
    │   │   │   └── utils/                  # (旧的GDAL通用辅助工具)
    │   │   │       └── (e.g., gdal_type_converter.h/cpp) # 🔗 迁移/合并逻辑至: src/readers/gdal/internal/utils/ (如 gdal_common_utils.h/cpp) 或被 `common_utils` 替代.
    │   │   │
    │   │   ├── netcdf/                     # (NetCDF 相关旧实现)
    │   │   │   ├── netcdf_cf_reader.h      # 🔗 迁移/合并逻辑至: src/readers/netcdf/netcdf_unified_reader.h/cpp (Phase 5). 逻辑被重构以适配新接口和新的时间处理.
    │   │   │   ├── netcdf_cf_reader.cpp    # 🔗 迁移/合并逻辑至: src/readers/netcdf/netcdf_unified_reader.h/cpp (Phase 5).
    │   │   │   ├── io/                     # (旧的NetCDF IO辅助组件)
    │   │   │   │   └── (e.g., netcdf_variable_reader.h/cpp) # 🔗 迁移/合并逻辑至: src/readers/netcdf/internal/io/ (如 netcdf_variable_io.h/cpp) 或直接整合进 netcdf_unified_reader.cpp.
    │   │   │   ├── parsing/                # (旧的NetCDF解析辅助组件)
    │   │   │   │   ├── netcdf_time_processor.h # ➡️ 删除 (Phase 4): 被 src/time/cf_time_extractor.h/cpp 取代.
    │   │   │   │   ├── netcdf_time_processor.cpp # ➡️ 删除 (Phase 4): 同上.
    │   │   │   │   └── netcdf_coordinate_system_parser.h # 🔄 重构并迁移逻辑至: src/readers/netcdf/internal/parsing/netcdf_coordinate_system_parser.h/cpp (Phase 4). 移除时间处理部分，专注空间坐标和维度解析.
    │   │   │   │   └── netcdf_coordinate_system_parser.cpp # 🔄 重构并迁移逻辑至: src/readers/netcdf/internal/parsing/netcdf_coordinate_system_parser.h/cpp (Phase 4).
    │   │   │   └── utils/                  # (旧的NetCDF通用辅助工具)
    │   │   │       └── (e.g., netcdf_attribute_utils.h/cpp) # 🔗 迁移/合并逻辑至: src/readers/netcdf/internal/ (如 netcdf_metadata_manager.h/cpp) 或被 `common_utils` 替代.
    │   │   │
    │   │   └── (其他可能的旧读取器类型如 HDF, GRIB, etc.) # 🔗 迁移/合并逻辑至: 如果支持这些格式，会创建新的对应的 UnifiedReader 实现 (e.g., src/readers/hdf/hdf_unified_reader.h/cpp)，并遵循相同的重构模式。
    │   │
    │   └── utils/                          # (旧的服务层级通用工具)
    │       └── (e.g., data_access_string_utils.h/cpp) # 🔗 迁移/合并逻辑至: 部分可能被 `common_utils/string` 等取代，部分特定逻辑可能内联到使用处，或如果仍必要则放入新的更具体的 `utils` 目录（如读取器内部的 `utils`）。大部分可能会被删除。
    │
    └── (其他可能的顶层src文件)             # 通常在良好组织的模块中较少，但如果存在，会根据其功能进行类似的删除/重构/迁移分析。

    core_services_impl/data_access_service/
├── include/core_services/data_access/  # ✨ 公共接口和核心类型 (清晰化、标准化)
│   ├── i_data_access_service.h         # (基本保留，新增流式API)
│   ├── common_types.h                  # ✨ 新增/重构: 统一存放模块级数据结构
│   ├── error_codes.h                   # ✨ 新增: 统一存放错误码
│   └── api/                              # ✨ 新增: 核心新接口定义
│       ├── i_data_source.h
│       ├── i_metadata_provider.h
│       ├── i_data_provider.h
│       ├── i_streaming_data_provider.h
│       ├── data_access_requests.h
│       ├── data_access_responses.h
│       └── streaming_types.h
│
└── src/                                  # ✨ 实现代码 (结构更清晰)
    ├── data_access_service_impl.h        # 🔄 重构: (原 raw_data_access_service_impl.h)
    ├── data_access_service_impl.cpp      # 🔄 重构: (原 raw_data_access_service_impl.cpp)
    ├── cache/                              # ✨ 新增: 统一缓存实现
    │   ├── unified_data_access_cache.h/cpp
    │   └── i_cacheable_value.h
    ├── async/                              # ✨ 新增: 统一异步执行器
    │   ├── unified_async_executor.h/cpp
    ├── time/                               # ✨ 新增: 时间处理相关
    │   ├── cf_time_extractor.h/cpp
    ├── readers/                            # ✨ 重构: 读取器相关实现 (统一架构)
    │   ├── core/                           # ✨ 新增: 读取器核心组件 (注册表、探测器、基类)
    │   │   ├── unified_data_reader.h/cpp
    │   │   ├── reader_registry.h/cpp
    │   │   ├── i_format_detector.h
    │   │   └── format_detector_impl.h/cpp
    │   ├── gdal/                           # ✨ 重构: GDAL 读取器 (基于新架构)
    │   │   ├── gdal_unified_reader.h/cpp
    │   │   └── internal/                   # (GDAL内部辅助件，内容重构)
    │   └── netcdf/                         # ✨ 重构: NetCDF 读取器 (基于新架构)
    │       ├── netcdf_unified_reader.h/cpp
    │       └── internal/                   # (NetCDF内部辅助件，内容重构)
    └── streaming/                          # ✨ 新增: 流式处理实现
        ├── streaming_data_processor.h/cpp

        core_services_impl/data_access_service/
├── cmake/                              # (可能的CMake相关配置)
│   └── (e.g., data_access_config.cmake)  # 🔗 迁移/合并逻辑至: 顶层或模块的CMakeLists.txt会更新以反映新的文件结构和依赖。旧的特定配置文件可能不再需要或被整合。
│
├── include/core_services/data_access/  # 对外公共头文件目录
│   ├── i_data_access_service.h         # 🔒 保留 (适配): 接口本身不变，但其实现类会被彻底重构。新增流式API。
│   │                                   #    新位置: include/core_services/data_access/i_data_access_service.h
│   ├── boost_future_config.h           # ➡️ 删除 (Phase 3): 被统一的异步执行器 UnifiedAsyncExecutor 取代。
│   ├── readers/                        # (旧的读取器相关公共头文件子目录)
│   │   └── data_reader_common.h        # ➡️ 删除 (Phase 2/5): 其内容（如通用枚举、简单结构体）可能部分迁移至新的 `common_types.h` 或被新API结构取代。
│   └── i_data_reader_impl.h            # ➡️ 删除 (Phase 5): 被新的API接口 (`IDataSource`, `IMetadataProvider`, `IDataProvider`, `IStreamingDataProvider`) 和 `UnifiedDataReader` 基类彻底取代。
│   └── (其他可能的旧公共头文件, e.g., specific_request_types.h, error_enums.h)
│                                       # 🔗 迁移/合并逻辑至: 相关的定义会迁移到新的 `api/data_access_requests.h`, `api/data_access_responses.h`, `common_types.h`, `error_codes.h`。
│
├── src/                                  # 源代码实现目录
│   ├── impl/                             # (通常包含大量的旧实现细节)
│   │   ├── raw_data_access_service_impl.h  # 🔄 重命名与重构至: src/data_access_service_impl.h (Phase 2-6) - 作为服务主实现类，但内部逻辑完全重写。
│   │   ├── raw_data_access_service_impl.cpp# 🔄 重命名与重构至: src/data_access_service_impl.cpp (Phase 2-6) - 同上。
│   │   │
│   │   ├── cache/                          # ➡️ 删除 (整个目录及其下所有文件在 Phase 2 被移除)
│   │   │   ├── data_chunk_cache.h/cpp      # ➡️ 删除: 被 src/cache/unified_data_access_cache.h/cpp 取代。
│   │   │   ├── reader_cache.h/cpp          # ➡️ 删除: 被 src/cache/unified_data_access_cache.h/cpp (用于缓存读取器实例) 取代。
│   │   │   ├── metadata_cache.h/cpp        # ➡️ 删除: 被 src/cache/unified_data_access_cache.h/cpp (用于缓存元数据) 取代。
│   │   │   ├── netcdf_cache_manager.h/cpp  # ➡️ 删除: NetCDF特定缓存逻辑统一到新缓存框架中。
│   │   │   └── cache_manager_template.h    # ➡️ 删除: 通用缓存模板被 `common_utils::cache` 和 `UnifiedDataAccessCache` 取代。
│   │   │
│   │   ├── factory/                        # ➡️ 删除 (整个目录及其下所有文件在 Phase 5 被移除)
│   │   │   ├── reader_factory.h/cpp        # ➡️ 删除: 被 src/readers/core/reader_registry.h/cpp 和 src/readers/core/format_detector_impl.h/cpp 取代。
│   │   │
│   │   ├── crs_service/                    # (如果 data_access 内部有对CRS的特定适配层或旧实现)
│   │   │   └── (e.g., gdal_crs_service_impl.h/cpp) # ➡️ 删除/🔗 迁移逻辑至: 如果是重复CRS逻辑，删除。如果是必要的适配，其逻辑可能融入 `GdalUnifiedReader` 或通过依赖注入的 `ICrsService` 实现。Phase 1-5强调了与外部 `crs_service` 的依赖注入和协调。
│   │   │
│   │   ├── readers/                        # (旧的具体读取器实现子目录)
│   │   │   ├── gdal/                       # (GDAL 相关旧实现)
│   │   │   │   ├── gdal_raster_reader.h/cpp    # 🔗 迁移/合并逻辑至: src/readers/gdal/gdal_unified_reader.h/cpp (Phase 5).
│   │   │   │   ├── gdal_vector_reader.h/cpp    # 🔗 迁移/合并逻辑至: src/readers/gdal/gdal_unified_reader.h/cpp (Phase 5).
│   │   │   │   ├── io/                     # 🔗 迁移/合并逻辑至: src/readers/gdal/internal/ (如 gdal_raster_io.h/cpp, gdal_vector_io.h/cpp).
│   │   │   │   │   └── (e.g., gdal_raster_io_utils.h/cpp)
│   │   │   │   ├── metadata/               # 🔗 迁移/合并逻辑至: src/readers/gdal/internal/ (如 gdal_metadata_extractor.h/cpp).
│   │   │   │   │   └── (e.g., gdal_attribute_reader.h/cpp)
│   │   │   │   └── utils/                  # 🔗 迁移/合并逻辑至: src/readers/gdal/internal/utils/ (如 gdal_common_utils.h/cpp) 或被 `common_utils` 替代.
│   │   │   │       └── (e.g., gdal_type_converter.h/cpp, gdal_error_handler.h/cpp)
│   │   │   │
│   │   │   ├── netcdf/                     # (NetCDF 相关旧实现)
│   │   │   │   ├── netcdf_cf_reader.h/cpp      # 🔗 迁移/合并逻辑至: src/readers/netcdf/netcdf_unified_reader.h/cpp (Phase 5).
│   │   │   │   ├── io/                     # 🔗 迁移/合并逻辑至: src/readers/netcdf/internal/io/ (如 netcdf_variable_io.h/cpp).
│   │   │   │   │   └── (e.g., netcdf_variable_reader.h/cpp)
│   │   │   │   ├── parsing/                # (旧的NetCDF解析辅助组件)
│   │   │   │   │   ├── netcdf_time_processor.h/cpp # ➡️ 删除 (Phase 4): 被 src/time/cf_time_extractor.h/cpp 取代.
│   │   │   │   │   ├── netcdf_coordinate_system_parser.h/cpp # 🔄 重构并迁移逻辑至: src/readers/netcdf/internal/parsing/netcdf_coordinate_system_parser.h/cpp (Phase 4). 移除时间处理部分.
│   │   │   │   │   └── (e.g., netcdf_attribute_parser.h/cpp) # 🔗 迁移/合并逻辑至: src/readers/netcdf/internal/netcdf_metadata_manager.h/cpp 或 parsing/.
│   │   │   │   └── utils/                  # 🔗 迁移/合并逻辑至: src/readers/netcdf/internal/ (或其下的 utils/) 或被 `common_utils` 替代.
│   │   │   │       └── (e.g., netcdf_attribute_utils.h/cpp)
│   │   │   │
│   │   │   └── (其他可能的旧读取器类型如 HDF, GRIB, etc.) # 🔗 迁移/合并逻辑至: 如果支持这些格式，会创建新的对应的 UnifiedReader 实现 (e.g., src/readers/hdf/hdf_unified_reader.h/cpp)，并遵循相同的重构模式。
│   │   │
│   │   └── utils/                          # (旧的服务层级通用工具)
│   │       └── (e.g., data_access_string_utils.h/cpp, data_conversion_helpers.h/cpp) # 🔗 迁移/合并逻辑至: 部分被 `common_utils` 取代，特定逻辑内联或移至更具体的 `utils` (如读取器内部)，大部分可能被删除。
│   │
│   └── (其他可能的顶层src文件或子目录，如 `common/`, `helpers/` 等)
│                                           # 🔗 迁移/合并逻辑至: 根据具体功能，大部分会被整合到新的结构化子目录 (`cache`, `async`, `time`, `readers`, `streaming`) 中，或被 `common_utils` 替代，或因冗余而被删除。
│
├── testdata/                             # 测试数据目录
│   ├── GRAY_LR_SR_W/                     # 🔒 保留 (测试数据，根据新测试需求可能调整或增删)
│   │   └── ... (files)
│   ├── ne_10m_admin_0_countries/         # 🔒 保留 (测试数据，根据新测试需求可能调整或增删)
│   │   └── ... (files)
│   └── (其他测试数据集)                    # 🔒 保留 (测试数据)
│
└── tests/                                # 测试代码目录
    ├── core_services/                    # (可能的子目录结构)
    │   └── (e.g., data_access_tests.cpp, gdal_reader_tests.cpp, netcdf_reader_tests.cpp)
    │                                     # 🗑️ 废弃/重写: 大部分旧的单元测试和集成测试需要重写，以适应新的API、类和依赖注入机制。
    │                                     #    新的测试将遵循AAA模式，针对 `UnifiedDataAccessCache`, `UnifiedAsyncExecutor`, `CFTimeExtractor`,
    │                                     #    各个 `UnifiedDataReader` 实现, `ReaderRegistry`, `StreamingDataProcessor` 以及
    │                                     #    `DataAccessServiceImpl` 的新逻辑编写。
    │                                     #    新测试文件会创建在类似 `tests/data_access/cache/`, `tests/data_access/async/`,
    │                                     #    `tests/data_access/readers/`, `tests/data_access/streaming/` 等新结构下。
    ├── mocks/                              # (可能的测试Mocks)
    │   └── (e.g., mock_gdal_bindings.h, mock_cache_user.h)
    │                                     # 🗑️ 废弃/重写: 旧的mocks可能不再适用。新的测试会使用如GMock等工具创建针对新接口的mocks
    │                                     #    (e.g., MockUnifiedCache, MockDataReader, MockCrsService, MockMetadataProvider).
    └── (其他测试辅助文件或主测试文件 main.cpp) # 🗑️ 废弃/重写 或 🔒 保留 (适配): 测试启动逻辑可能保留，但测试用例本身会大改。