/**
 * @file streaming_processor.cpp
 * @brief æµå¼æ•°æ®å¤„ç†å™¨å®ç° - ç®€åŒ–ç‰ˆ
 */

#include "streaming_processor.h"
#include "common_utils/utilities/logging_utils.h"
#include "../readers/core/unified_data_reader.h"

#include <boost/thread/future.hpp>
#include <boost/asio/post.hpp>
#include <algorithm>
#include <cmath>

namespace oscean::core_services::data_access::streaming {

// =============================================================================
// AdaptiveChunkingStrategy å®ç°
// =============================================================================

AdaptiveChunkingStrategy::AdaptiveChunkingStrategy(
    size_t minChunkSize,
    size_t maxChunkSize,
    size_t targetChunkSize)
    : minChunkSize_(minChunkSize)
    , maxChunkSize_(maxChunkSize)
    , targetChunkSize_(targetChunkSize)
    , currentChunkSize_(targetChunkSize) {
}

size_t AdaptiveChunkingStrategy::adjustChunkSize(
    std::chrono::milliseconds processingTime,
    double memoryPressure) {
    
    // è®°å½•å¤„ç†æ—¶é—´å†å²
    processingHistory_.push_back(processingTime);
    if (processingHistory_.size() > MAX_HISTORY_SIZE) {
        processingHistory_.erase(processingHistory_.begin());
    }
    
    // è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
    double avgTime = getAverageProcessingTime();
    
    // æ ¹æ®å¤„ç†æ—¶é—´å’Œå†…å­˜å‹åŠ›è°ƒæ•´å—å¤§å°
    if (avgTime > 1000.0) { // è¶…è¿‡1ç§’ï¼Œå‡å°å—å¤§å°
        currentChunkSize_ = std::max(minChunkSize_, 
                                   static_cast<size_t>(currentChunkSize_ * 0.8));
    } else if (avgTime < 100.0 && memoryPressure < 0.5) { // å¤„ç†å¿«ä¸”å†…å­˜å……è¶³ï¼Œå¢å¤§å—å¤§å°
        currentChunkSize_ = std::min(maxChunkSize_, 
                                   static_cast<size_t>(currentChunkSize_ * 1.2));
    }
    
    return currentChunkSize_;
}

void AdaptiveChunkingStrategy::reset() {
    currentChunkSize_ = targetChunkSize_;
    processingHistory_.clear();
}

double AdaptiveChunkingStrategy::getAverageProcessingTime() const {
    if (processingHistory_.empty()) {
        return 0.0;
    }
    
    double total = 0.0;
    for (const auto& time : processingHistory_) {
        total += time.count();
    }
    return total / processingHistory_.size();
}

// =============================================================================
// BackpressureController å®ç°
// =============================================================================

BackpressureController::BackpressureController(
    size_t maxQueueSize,
    double backpressureThreshold,
    bool dropOnOverflow)
    : maxQueueSize_(maxQueueSize)
    , backpressureThreshold_(backpressureThreshold)
    , dropOnOverflow_(dropOnOverflow) {
}

bool BackpressureController::shouldApplyBackpressure(size_t currentQueueSize) const {
    return currentQueueSize >= static_cast<size_t>(maxQueueSize_ * backpressureThreshold_);
}

boost::future<bool> BackpressureController::waitForCapacity(
    size_t currentQueueSize, 
    std::chrono::milliseconds timeout) {
    
    return boost::async(boost::launch::async, [this, currentQueueSize, timeout]() -> bool {
        if (!shouldApplyBackpressure(currentQueueSize)) {
            return true;
        }
        
        std::unique_lock<std::mutex> lock(mutex_);
        return capacityCondition_.wait_for(lock, timeout, [this, currentQueueSize]() {
            return !shouldApplyBackpressure(currentQueueSize);
        });
    });
}

void BackpressureController::notifyCapacityChange() {
    std::lock_guard<std::mutex> lock(mutex_);
    capacityCondition_.notify_all();
}

// =============================================================================
// StreamingProcessor å®ç°
// =============================================================================

StreamingProcessor::StreamingProcessor()
    : isInitialized_(false)
    , defaultChunkSize_(1024 * 1024) // 1MBé»˜è®¤åˆ†å—å¤§å°
    , maxConcurrentChunks_(8)
    , backpressureThreshold_(0.8) // 80%çš„ç¼“å†²åŒºä½¿ç”¨ç‡è§¦å‘èƒŒå‹
    , processedChunks_(0)
    , failedChunks_(0)
    , currentConcurrency_(0) {
    LOG_INFO("åˆ›å»ºStreamingProcessor");
}

StreamingProcessor::~StreamingProcessor() {
    LOG_INFO("StreamingProcessorææ„");
    shutdown();
}

void StreamingProcessor::initialize() {
    if (isInitialized_.load()) {
        return;
    }
    
    try {
        // ğŸ”§ æ£€æŸ¥æ˜¯å¦ä¸ºå•çº¿ç¨‹æ¨¡å¼
        const char* runMode = std::getenv("OSCEAN_RUN_MODE");
        bool isSingleThreadMode = (runMode && std::string(runMode) == "SINGLE_THREAD");
        
        if (isSingleThreadMode) {
            // å•çº¿ç¨‹æ¨¡å¼ï¼šä¸åˆ›å»ºçº¿ç¨‹æ± 
            threadPool_ = nullptr;
            LOG_INFO("StreamingProcessorè¿è¡Œåœ¨å•çº¿ç¨‹æ¨¡å¼ï¼Œä¸åˆ›å»ºçº¿ç¨‹æ± ");
        } else {
            // ç”Ÿäº§æ¨¡å¼ï¼šåˆ›å»ºçº¿ç¨‹æ± ï¼Œä½†é™åˆ¶å¤§å°
            size_t poolSize = std::min(maxConcurrentChunks_, size_t{4});  // æœ€å¤š4çº¿ç¨‹
            threadPool_ = std::make_unique<boost::asio::thread_pool>(poolSize);
            LOG_INFO("StreamingProcessoråˆ›å»ºçº¿ç¨‹æ± ï¼Œå¤§å°: {}", poolSize);
        }
        
        isInitialized_.store(true);
        LOG_INFO("StreamingProcessoråˆå§‹åŒ–å®Œæˆ");
        
    } catch (const std::exception& e) {
        LOG_ERROR("StreamingProcessoråˆå§‹åŒ–å¤±è´¥: {}", e.what());
        throw;
    }
}

void StreamingProcessor::shutdown() {
    if (!isInitialized_.load()) {
        return;
    }
    
    try {
        isInitialized_.store(false);
        
        // åœæ­¢çº¿ç¨‹æ± 
        if (threadPool_) {
            threadPool_->stop();
            threadPool_->join();
            threadPool_.reset();
        }
        
        LOG_INFO("StreamingProcessorå…³é—­å®Œæˆ");
        
    } catch (const std::exception& e) {
        LOG_ERROR("StreamingProcessorå…³é—­å¼‚å¸¸: {}", e.what());
    }
}

boost::future<void> StreamingProcessor::processStreamAsync(
    std::shared_ptr<readers::UnifiedDataReader> reader,
    const std::string& variableName,
    std::function<boost::future<bool>(ProcessingDataChunk)> chunkProcessor,
    const StreamingOptions& options) {
    
    if (!isInitialized_.load()) {
        auto exception = std::runtime_error("StreamingProcessoræœªåˆå§‹åŒ–");
        std::exception_ptr eptr = std::make_exception_ptr(exception);
        boost::promise<void> promise;
        promise.set_exception(eptr);
        return promise.get_future();
    }
    
    if (!reader) {
        auto exception = std::invalid_argument("readerä¸èƒ½ä¸ºnullptr");
        std::exception_ptr eptr = std::make_exception_ptr(exception);
        boost::promise<void> promise;
        promise.set_exception(eptr);
        return promise.get_future();
    }
    
    if (!chunkProcessor) {
        auto exception = std::invalid_argument("chunkProcessorä¸èƒ½ä¸ºnullptr");
        std::exception_ptr eptr = std::make_exception_ptr(exception);
        boost::promise<void> promise;
        promise.set_exception(eptr);
        return promise.get_future();
    }
    
    return boost::async(boost::launch::async, [this, reader, variableName, chunkProcessor, options]() {
        processStreamImpl(reader, variableName, chunkProcessor, options);
    });
}

void StreamingProcessor::setChunkSize(size_t chunkSize) {
    if (chunkSize > 0) {
        defaultChunkSize_ = chunkSize;
        LOG_INFO("è®¾ç½®é»˜è®¤åˆ†å—å¤§å°: {}", chunkSize);
    }
}

void StreamingProcessor::setMaxConcurrentChunks(size_t maxChunks) {
    if (maxChunks > 0) {
        maxConcurrentChunks_ = maxChunks;
        LOG_INFO("è®¾ç½®æœ€å¤§å¹¶å‘åˆ†å—æ•°: {}", maxChunks);
    }
}

void StreamingProcessor::setBackpressureThreshold(double threshold) {
    if (threshold > 0.0 && threshold <= 1.0) {
        backpressureThreshold_ = threshold;
        LOG_INFO("è®¾ç½®èƒŒå‹é˜ˆå€¼: {}", threshold);
    }
}

StreamingStatistics StreamingProcessor::getStatistics() const {
    StreamingStatistics stats;
    stats.processedChunks = processedChunks_.load();
    stats.failedChunks = failedChunks_.load();
    stats.currentConcurrency = currentConcurrency_.load();
    stats.maxConcurrency = maxConcurrentChunks_;
    stats.averageChunkSize = calculateAverageChunkSize();
    stats.successRate = calculateSuccessRate();
    
    return stats;
}

bool StreamingProcessor::startStreaming(const std::string& streamId) {
    std::unique_lock<std::shared_mutex> lock(streamsMutex_);
    
    if (activeStreams_.find(streamId) != activeStreams_.end()) {
        LOG_WARN("æµå·²å­˜åœ¨: {}", streamId);
        return false;
    }
    
    activeStreams_[streamId] = StreamState::ACTIVE;
    streamStats_[streamId] = StreamStatistics{};
    
    LOG_INFO("å¼€å§‹æµå¤„ç†: {}", streamId);
    return true;
}

bool StreamingProcessor::stopStreaming(const std::string& streamId) {
    std::unique_lock<std::shared_mutex> lock(streamsMutex_);
    
    auto it = activeStreams_.find(streamId);
    if (it == activeStreams_.end()) {
        LOG_WARN("æµä¸å­˜åœ¨: {}", streamId);
        return false;
    }
    
    it->second = StreamState::COMPLETED;
    LOG_INFO("åœæ­¢æµå¤„ç†: {}", streamId);
    return true;
}

boost::future<bool> StreamingProcessor::submitChunk(const std::string& streamId, ProcessingDataChunk chunk) {
    return boost::async(boost::launch::async, [this, streamId, chunk]() -> bool {
        std::shared_lock<std::shared_mutex> lock(streamsMutex_);
        
        auto it = activeStreams_.find(streamId);
        if (it == activeStreams_.end() || it->second != StreamState::ACTIVE) {
            LOG_WARN("æµä¸æ´»è·ƒæˆ–ä¸å­˜åœ¨: {}", streamId);
            return false;
        }
        
        try {
            // ç®€åŒ–çš„å—å¤„ç†é€»è¾‘
            LOG_DEBUG("å¤„ç†æ•°æ®å—: stream={}, index={}, size={}", 
                     streamId, chunk.chunkIndex, chunk.data.size());
            
            // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            {
                std::unique_lock<std::shared_mutex> statsLock(streamsMutex_);
                auto& stats = streamStats_[streamId];
                stats.totalChunksProcessed++;
                stats.totalBytesProcessed += chunk.data.size() * sizeof(double);
                stats.updateThroughput();
            }
            
            return true;
            
        } catch (const std::exception& e) {
            LOG_ERROR("å¤„ç†æ•°æ®å—å¼‚å¸¸: {} - {}", streamId, e.what());
            return false;
        }
    });
}

StreamStatistics StreamingProcessor::getStreamStatistics(const std::string& streamId) const {
    std::shared_lock<std::shared_mutex> lock(streamsMutex_);
    
    auto it = streamStats_.find(streamId);
    if (it != streamStats_.end()) {
        return it->second;
    }
    
    return StreamStatistics{};
}

void StreamingProcessor::processStreamImpl(
    std::shared_ptr<readers::UnifiedDataReader> reader,
    const std::string& variableName,
    std::function<boost::future<bool>(ProcessingDataChunk)> chunkProcessor,
    const StreamingOptions& options) {
    
    try {
        // è®¡ç®—åˆ†å—ç­–ç•¥
        auto chunkingStrategy = calculateChunkingStrategy(variableName, reader, options);
        
        // æµå¼è¯»å–å’Œå¤„ç†æ•°æ®
        size_t totalChunks = chunkingStrategy.totalChunks;
        std::vector<boost::future<bool>> chunkFutures;
        chunkFutures.reserve(totalChunks);
        
        for (size_t chunkIndex = 0; chunkIndex < totalChunks; ++chunkIndex) {
            // æ£€æŸ¥èƒŒå‹
            if (shouldApplyBackpressure()) {
                waitForBackpressureRelief();
            }
            
            // åˆ›å»ºåˆ†å—ä»»åŠ¡
            auto chunkFuture = createChunkTask(reader, variableName, chunkIndex, chunkingStrategy, chunkProcessor);
            chunkFutures.push_back(std::move(chunkFuture));
            
            currentConcurrency_.fetch_add(1);
        }
        
        // ç­‰å¾…æ‰€æœ‰åˆ†å—å®Œæˆ
        for (auto& future : chunkFutures) {
            try {
                bool success = future.get();
                if (success) {
                    processedChunks_.fetch_add(1);
                } else {
                    failedChunks_.fetch_add(1);
                }
            } catch (const std::exception& e) {
                LOG_ERROR("åˆ†å—å¤„ç†å¼‚å¸¸: {}", e.what());
                failedChunks_.fetch_add(1);
            }
            currentConcurrency_.fetch_sub(1);
        }
        
        LOG_INFO("æµå¼å¤„ç†å®Œæˆ: {} chunks processed, {} failed", 
                processedChunks_.load(), failedChunks_.load());
        
    } catch (const std::exception& e) {
        LOG_ERROR("æµå¼å¤„ç†å¤±è´¥ [{}::{}]: {}", reader->getFilePath(), variableName, e.what());
        throw;
    }
}

ChunkingStrategy StreamingProcessor::calculateChunkingStrategy(
    const std::string& variableName,
    std::shared_ptr<readers::UnifiedDataReader> reader,
    const StreamingOptions& options) {
    
    ChunkingStrategy strategy;
    
    try {
        // ä¼°ç®—å˜é‡å¤§å°
        size_t estimatedSize = estimateVariableSize(variableName, reader);
        
        // è®¡ç®—åˆ†å—å‚æ•°
        size_t chunkSize = options.chunkSize > 0 ? options.chunkSize : defaultChunkSize_;
        strategy.chunkSize = chunkSize;
        strategy.totalChunks = (estimatedSize + chunkSize - 1) / chunkSize; // å‘ä¸Šå–æ•´
        strategy.lastChunkSize = estimatedSize % chunkSize;
        if (strategy.lastChunkSize == 0) {
            strategy.lastChunkSize = chunkSize;
        }
        strategy.chunkingType = ChunkingType::SEQUENTIAL;
        
        LOG_DEBUG("åˆ†å—ç­–ç•¥: æ€»å¤§å°={}, å—å¤§å°={}, æ€»å—æ•°={}", 
                 estimatedSize, chunkSize, strategy.totalChunks);
        
    } catch (const std::exception& e) {
        LOG_ERROR("è®¡ç®—åˆ†å—ç­–ç•¥å¤±è´¥: {}", e.what());
        // ä½¿ç”¨é»˜è®¤ç­–ç•¥
        strategy.chunkSize = defaultChunkSize_;
        strategy.totalChunks = 1;
        strategy.lastChunkSize = defaultChunkSize_;
        strategy.chunkingType = ChunkingType::SEQUENTIAL;
    }
    
    return strategy;
}

boost::future<bool> StreamingProcessor::createChunkTask(
    std::shared_ptr<readers::UnifiedDataReader> reader,
    const std::string& variableName,
    size_t chunkIndex,
    const ChunkingStrategy& strategy,
    std::function<boost::future<bool>(ProcessingDataChunk)> chunkProcessor) {
    
    return boost::async(boost::launch::async, [this, reader, variableName, chunkIndex, strategy, chunkProcessor]() -> bool {
        try {
            // è®¡ç®—å½“å‰å—çš„èŒƒå›´
            auto range = calculateChunkRange(chunkIndex, strategy);
            
            // è¯»å–å—æ•°æ®
            auto chunkData = readChunkDataComplete(reader, variableName, range, strategy);
            if (!chunkData) {
                LOG_ERROR("è¯»å–å—æ•°æ®å¤±è´¥: chunk={}", chunkIndex);
                return false;
            }
            
            // åˆ›å»ºProcessingDataChunkå¯¹è±¡
            ProcessingDataChunk chunk;
            chunk.data = *chunkData;
            chunk.chunkIndex = chunkIndex;
            chunk.totalChunks = strategy.totalChunks;
            chunk.isLastChunk = (chunkIndex == strategy.totalChunks - 1);
            
            // è®¾ç½®DataChunkKey
            chunk.chunkKey = oscean::core_services::DataChunkKey(
                "", variableName, boost::none, boost::none, boost::none, "double");
            
            // å¤„ç†å—
            auto processFuture = chunkProcessor(chunk);
            return processFuture.get();
            
        } catch (const std::exception& e) {
            LOG_ERROR("åˆ›å»ºåˆ†å—ä»»åŠ¡å¼‚å¸¸: chunk={} - {}", chunkIndex, e.what());
            return false;
        }
    });
}

bool StreamingProcessor::shouldApplyBackpressure() const {
    return currentConcurrency_.load() >= static_cast<size_t>(maxConcurrentChunks_ * backpressureThreshold_);
}

void StreamingProcessor::waitForBackpressureRelief() {
    while (shouldApplyBackpressure()) {
        std::this_thread::sleep_for(std::chrono::milliseconds(10));
    }
}

size_t StreamingProcessor::estimateVariableSize(const std::string& variableName, 
                                               std::shared_ptr<readers::UnifiedDataReader> reader) const {
    // ç®€åŒ–å®ç°ï¼šè¿”å›é»˜è®¤å¤§å°
    return 10 * 1024 * 1024; // 10MB
}

size_t StreamingProcessor::getDataTypeSize(oscean::core_services::DataType dataType) const {
    switch (dataType) {
        case oscean::core_services::DataType::Byte:
            return sizeof(uint8_t);
        case oscean::core_services::DataType::Int16:
            return sizeof(int16_t);
        case oscean::core_services::DataType::UInt16:
            return sizeof(uint16_t);
        case oscean::core_services::DataType::Int32:
            return sizeof(int32_t);
        case oscean::core_services::DataType::UInt32:
            return sizeof(uint32_t);
        case oscean::core_services::DataType::Float32:
            return sizeof(float);
        case oscean::core_services::DataType::Float64:
            return sizeof(double);
        default:
            return sizeof(double); // é»˜è®¤
    }
}

oscean::core_services::IndexRange StreamingProcessor::calculateChunkRange(size_t chunkIndex, const ChunkingStrategy& strategy) const {
    oscean::core_services::IndexRange range;
    range.start = static_cast<int>(chunkIndex * strategy.chunkSize);
    
    if (chunkIndex == strategy.totalChunks - 1) {
        // æœ€åä¸€å—
        range.count = static_cast<int>(strategy.lastChunkSize);
    } else {
        range.count = static_cast<int>(strategy.chunkSize);
    }
    
    // IndexRangeä¸éœ€è¦endIndexå­—æ®µï¼Œå·²ç»æœ‰startå’Œcount
    
    return range;
}

std::unique_ptr<std::vector<double>> StreamingProcessor::readChunkDataComplete(
    std::shared_ptr<readers::UnifiedDataReader> reader,
    const std::string& variableName,
    const oscean::core_services::IndexRange& range,
    const ChunkingStrategy& strategy) const {
    
    try {
        // ç®€åŒ–å®ç°ï¼šåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        auto data = std::make_unique<std::vector<double>>(range.count);
        
        // å¡«å……æ¨¡æ‹Ÿæ•°æ®
        for (size_t i = 0; i < range.count; ++i) {
            (*data)[i] = static_cast<double>(range.start + i) * 0.1;
        }
        
        return data;
        
    } catch (const std::exception& e) {
        LOG_ERROR("è¯»å–å—æ•°æ®å¼‚å¸¸: {} - {}", variableName, e.what());
        return nullptr;
    }
}

std::unique_ptr<std::vector<double>> StreamingProcessor::readSpatialChunk(
    std::shared_ptr<readers::UnifiedDataReader> reader,
    const std::string& variableName,
    const StreamDataRange& range) const {
    
    // ç®€åŒ–å®ç°ï¼šå§”æ‰˜ç»™å®Œæ•´è¯»å–æ–¹æ³•
    ChunkingStrategy strategy;
    strategy.chunkSize = range.count;
    return readChunkDataComplete(reader, variableName, range, strategy);
}

std::unique_ptr<std::vector<double>> StreamingProcessor::readSequentialChunk(
    std::shared_ptr<readers::UnifiedDataReader> reader,
    const std::string& variableName,
    const StreamDataRange& range) const {
    
    // ç®€åŒ–å®ç°ï¼šå§”æ‰˜ç»™å®Œæ•´è¯»å–æ–¹æ³•
    ChunkingStrategy strategy;
    strategy.chunkSize = range.count;
    return readChunkDataComplete(reader, variableName, range, strategy);
}

double StreamingProcessor::calculateAverageChunkSize() const {
    size_t totalChunks = processedChunks_.load() + failedChunks_.load();
    if (totalChunks == 0) {
        return 0.0;
    }
    return static_cast<double>(defaultChunkSize_);
}

double StreamingProcessor::calculateSuccessRate() const {
    size_t totalChunks = processedChunks_.load() + failedChunks_.load();
    if (totalChunks == 0) {
        return 1.0;
    }
    return static_cast<double>(processedChunks_.load()) / totalChunks;
}

} // namespace oscean::core_services::data_access::streaming 
